
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Introduction to PyTorch &#8212; ùó¶ùòÅùó≤ùó≤ùóΩùó≤ùòÄùòÅ ùóîùòÄùó∞ùó≤ùóªùòÅ ‚õ∞Ô∏è</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="shortcut icon" href="../_static/loss_surface.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Activation Functions" href="pytorch-activation.html" />
    <link rel="prev" title="Mechanics of TensorFlow" href="seb3/tensorflow-mechanics.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/loss_surface.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">ùó¶ùòÅùó≤ùó≤ùóΩùó≤ùòÄùòÅ ùóîùòÄùó∞ùó≤ùóªùòÅ ‚õ∞Ô∏è</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  FUNDAMENTALS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="house-prices.html">
   Pipelines in Scikit-Learn
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="blending-stacking.html">
   Blending and Stacking
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optuna.html">
   Hyperparameter Tuning with Optuna
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  DEEP LEARNING
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="backpropagation.html">
   Backpropagation on DAGs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="seb3/tensorflow-nn.html">
   Neural Networks with TensorFlow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="seb3/tensorflow-mechanics.html">
   Mechanics of TensorFlow
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  PYTORCH TUTORIALS
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Introduction to PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pytorch-activation.html">
   Activation Functions
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  REST APIs with FASTAPI
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="fastapi/ch3.html">
   Developing a RESTful API
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="fastapi/ch4.html">
   Managing Pydantic Data Models
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/pytorch-intro.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        <a class="issues-button"
            href="https://github.com/particle1331/steepest-ascent/issues/new?title=Issue%20on%20page%20%2Fnotebooks/pytorch-intro.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/particle1331/steepest-ascent/master?urlpath=tree/docs/notebooks/pytorch-intro.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basics-of-pytorch">
   Basics of PyTorch
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tensors">
     Tensors
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#initialization">
       Initialization
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#converting-tensor-leftrightarrow-numpy">
       Converting: tensor
       <span class="math notranslate nohighlight">
        \(\leftrightarrow\)
       </span>
       numpy
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#operations">
       Operations
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#indexing">
       Indexing
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computational-graphs-and-backpropagation">
     Computational Graphs and Backpropagation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gpu-support">
     GPU Support
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-the-continuous-xor">
   Learning the Continuous XOR
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model">
     Model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#nn-module">
       nn.Module
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#simple-classifier">
       Simple classifier
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data">
     Data
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-dataset-class">
       The
       <code class="docutils literal notranslate">
        <span class="pre">
         Dataset
        </span>
       </code>
       class
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-dataloader-class">
       The
       <code class="docutils literal notranslate">
        <span class="pre">
         DataLoader
        </span>
       </code>
       class
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimization">
     Optimization
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#loss-modules">
       Loss modules
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#stochastic-gradient-descent">
       Stochastic Gradient Descent
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training">
     Training
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#saving-a-model">
       Saving a model
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluation">
     Evaluation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#visualizing-classification-boundaries">
       Visualizing classification boundaries
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tensorboard-logging">
   TensorBoard Logging
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendix-einstein-summation">
   Appendix: Einstein summation
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="introduction-to-pytorch">
<h1>Introduction to PyTorch<a class="headerlink" href="#introduction-to-pytorch" title="Permalink to this headline">¬∂</a></h1>
<div class="admonition-attribution admonition">
<p class="admonition-title">Attribution</p>
<p>This notebook is based on <a class="reference external" href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.html">Tutorial 2</a> of the <a class="reference external" href="https://uvadlc.github.io/lectures-nov2020.html">Deep Learning Course</a> at the University of Amsterdam. The full list of tutorials can be found <a class="reference external" href="https://uvadlc-notebooks.rtfd.io">here</a>.</p>
</div>
<p>The following notebook is meant to give a short introduction to PyTorch basics, and get you setup for writing your own neural networks. <strong>PyTorch</strong> is an open source machine learning framework that allows you to write your own neural networks and optimize them efficiently. However, PyTorch is not the only framework of its kind. Alternatives to PyTorch include TensorFlow, JAX and MXNet. We choose PyTorch because it is well established, has a huge developer community (originally developed by Facebook), is very flexible and especially used in research. Many current papers publish their code in PyTorch, and thus it is good to be familiar with PyTorch as well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">to_rgba</span>

<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="basics-of-pytorch">
<h2>Basics of PyTorch<a class="headerlink" href="#basics-of-pytorch" title="Permalink to this headline">¬∂</a></h2>
<p>We will start with reviewing the very basic concepts of PyTorch. As a first step, let us check the version of the installed PyTorch library <code class="docutils literal notranslate"><span class="pre">torch</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Using torch&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Using torch 1.9.1
</pre></div>
</div>
</div>
</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Setting the seed</strong></p>
</div>
<p>As in every machine learning framework, PyTorch provides functions that are stochastic like generating random numbers. However, a very good practice is to setup your code to be reproducible with the exact same random numbers. This is why we set a seed below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span> <span class="c1"># Setting the seed</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;torch._C.Generator at 0x7f0b58423f30&gt;
</pre></div>
</div>
</div>
</div>
<div class="section" id="tensors">
<h3>Tensors<a class="headerlink" href="#tensors" title="Permalink to this headline">¬∂</a></h3>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Tensors = numpy arrays + GPU support</strong></p>
</div>
<p>Tensors are the PyTorch equivalent to NumPy arrays, with the addition to also have support for GPU acceleration (more on that later).
Hence, as a prerequisite, we recommend to be familiar with the <code class="docutils literal notranslate"><span class="pre">numpy</span></code> package as most machine learning frameworks are based on very similar concepts. In this case, the name ‚Äútensor‚Äù is a generalization of concepts you already know. For instance, a vector is a 1-D tensor, and a matrix a 2-D tensor. When working with neural networks, we will use tensors of various shapes and number of dimensions.</p>
<p>Most common functions you know from NumPy can be used on tensors as well. Actually, since NumPy arrays are so similar to tensors, we can convert most tensors to NumPy arrays (and back) but we don‚Äôt need it too often.</p>
<div class="section" id="initialization">
<h4>Initialization<a class="headerlink" href="#initialization" title="Permalink to this headline">¬∂</a></h4>
<p>Let‚Äôs first start by looking at different ways of creating a tensor. There are many possible options, the most simple one is to call <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> with the desired shape as input argument:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[2.5353e+30, 9.5288e-44, 2.8500e-41, 7.3877e-39],
         [6.4961e+10, 2.5353e+30, 1.1491e-43, 2.9935e-41],
         [4.3845e+31, 7.5338e+28, 2.9633e+29, 8.3346e+00]],

        [[4.8251e+27, 1.3981e+19, 4.7037e+30, 4.3440e-44],
         [1.1280e-41, 2.9796e-39, 1.1652e-32, 1.3744e+11],
         [1.0141e+31, 7.0065e-44, 4.3536e-19, 2.5715e-38]]])
</pre></div>
</div>
</div>
</div>
<p>The constructor <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">torch.empty</span></code> which allocates memory for the desired tensor, but reuses any values that have already been in the memory. To create a tensor from data we can use <code class="docutils literal notranslate"><span class="pre">torch.tensor(data,</span> <span class="pre">dtype=None)</span></code> which returns a tensor version of the data with the appropriate type.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a tensor from data</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1, 2],
        [3, 4]])
</pre></div>
</div>
</div>
</div>
<p>For tensors with specific values, e.g. ones and zeros, we can use <code class="docutils literal notranslate"><span class="pre">torch.zeros</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.ones</span></code> instead of passing explicit data. Similarly, we can use <code class="docutils literal notranslate"><span class="pre">torch.rand</span></code> to sample from the uniform distribution on <span class="math notranslate nohighlight">\([0, 1]\)</span>, and <code class="docutils literal notranslate"><span class="pre">torch.randn</span></code> to sample from the standard normal. These functions take in <code class="docutils literal notranslate"><span class="pre">*args</span></code> such that the resulting tensor has shape <code class="docutils literal notranslate"><span class="pre">args</span></code>. Finally, <code class="docutils literal notranslate"><span class="pre">torch.arange(m,</span> <span class="pre">n,</span> <span class="pre">step=h)</span></code> initializes a tensor of equally spaced numbers in <code class="docutils literal notranslate"><span class="pre">[m,</span> <span class="pre">n)</span></code> with step-size <code class="docutils literal notranslate"><span class="pre">h</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a tensor with random values between 0 and 1 with the shape [2, 3, 4]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[0.8823, 0.9150, 0.3829, 0.9593],
         [0.3904, 0.6009, 0.2566, 0.7936],
         [0.9408, 0.1332, 0.9346, 0.5936]],

        [[0.8694, 0.5677, 0.7411, 0.4294],
         [0.8854, 0.5739, 0.2666, 0.6274],
         [0.2696, 0.4414, 0.2969, 0.8317]]])
</pre></div>
</div>
</div>
</div>
<p>You can obtain the shape of a tensor in the same way as in NumPy (<code class="docutils literal notranslate"><span class="pre">x.shape</span></code>), or using the <code class="docutils literal notranslate"><span class="pre">.size</span></code> method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Size:&quot;</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>

<span class="n">dim1</span><span class="p">,</span> <span class="n">dim2</span><span class="p">,</span> <span class="n">dim3</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Size:&quot;</span><span class="p">,</span> <span class="n">dim1</span><span class="p">,</span> <span class="n">dim2</span><span class="p">,</span> <span class="n">dim3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape: torch.Size([2, 3, 4])
Size: torch.Size([2, 3, 4])
Size: 2 3 4
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="converting-tensor-leftrightarrow-numpy">
<h4>Converting: tensor <span class="math notranslate nohighlight">\(\leftrightarrow\)</span> numpy<a class="headerlink" href="#converting-tensor-leftrightarrow-numpy" title="Permalink to this headline">¬∂</a></h4>
<p>Tensors can be converted to numpy arrays, and numpy arrays back to tensors. To transform a numpy array into a tensor, we can use the function <code class="docutils literal notranslate"><span class="pre">torch.from_numpy</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np_arr</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;NumPy array:&quot;</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">np_arr</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">PyTorch tensor:&quot;</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>NumPy array:
[[1 2]
 [3 4]]

PyTorch tensor:
tensor([[1, 2],
        [3, 4]])
</pre></div>
</div>
</div>
</div>
<p>To transform a PyTorch tensor back to a numpy array, we can use the function <code class="docutils literal notranslate"><span class="pre">.numpy()</span></code> on tensors:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">np_arr</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PyTorch tensor:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;NumPy array:&quot;</span><span class="p">,</span> <span class="n">np_arr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>PyTorch tensor: tensor([0, 1, 2, 3])
NumPy array: [0 1 2 3]
</pre></div>
</div>
</div>
</div>
<p>The conversion of tensors to numpy require the tensor to be on the CPU, and not the GPU (more on GPU support in a later section). In case you have a tensor on GPU, you need to call <code class="docutils literal notranslate"><span class="pre">.cpu()</span></code> on the tensor beforehand. Hence, you get a line like <code class="docutils literal notranslate"><span class="pre">np_arr</span> <span class="pre">=</span> <span class="pre">tensor.cpu().numpy()</span></code>.</p>
</div>
<div class="section" id="operations">
<h4>Operations<a class="headerlink" href="#operations" title="Permalink to this headline">¬∂</a></h4>
<p>Most operations that exist in numpy, also exist in PyTorch. A full list of operations can be found in the <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html">PyTorch documentation</a>, but we will review the most important ones here.</p>
<p>The simplest operation is to add two tensors:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x1 =&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x2 =&quot;</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y =&quot;</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x1 =
tensor([[0.1053, 0.2695, 0.3588],
        [0.1994, 0.5472, 0.0062]])
x2 =
tensor([[0.9516, 0.0753, 0.8860],
        [0.5832, 0.3376, 0.8090]])

y =
tensor([[1.0569, 0.3448, 1.2448],
        [0.7826, 0.8848, 0.8151]])
</pre></div>
</div>
</div>
</div>
<p>Calling <code class="docutils literal notranslate"><span class="pre">x1</span> <span class="pre">+</span> <span class="pre">x2</span></code> creates a new tensor containing the sum of the two inputs. However, we can also use in-place operations that are applied directly on the memory of a tensor. We therefore change the values of <code class="docutils literal notranslate"><span class="pre">x2</span></code> without the chance to re-accessing the values of <code class="docutils literal notranslate"><span class="pre">x2</span></code> before the operation. An example is shown below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x1 (before):&quot;</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">x2 (before):&quot;</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---&#39;</span><span class="p">)</span>

<span class="n">x2</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">x1 (after):&quot;</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">x2 (after):&quot;</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x1 (before):
tensor([[0.5779, 0.9040, 0.5547],
        [0.3423, 0.6343, 0.3644]])

x2 (before):
tensor([[0.7104, 0.9464, 0.7890],
        [0.2814, 0.7886, 0.5895]])

---

x1 (after):
tensor([[0.5779, 0.9040, 0.5547],
        [0.3423, 0.6343, 0.3644]])

x2 (after):
tensor([[1.2884, 1.8504, 1.3437],
        [0.6237, 1.4230, 0.9539]])
</pre></div>
</div>
</div>
</div>
<p>In-place operations are usually marked with a underscore postfix (e.g. <code class="docutils literal notranslate"><span class="pre">add_</span></code> instead of <code class="docutils literal notranslate"><span class="pre">add</span></code>).</p>
<p>Another common operation aims at changing the shape of a tensor. A tensor of size <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">3)</span></code> can be re-organized to any other shape with the same number of elements (e.g. a tensor of size <code class="docutils literal notranslate"><span class="pre">(6,)</span></code>, or <code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">2)</span></code>, ‚Ä¶). In PyTorch, this operation is called <code class="docutils literal notranslate"><span class="pre">view</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0, 1, 2, 3, 4, 5])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0, 1, 2],
        [3, 4, 5]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># Swapping dimension 0 and 1</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0, 3],
        [1, 4],
        [2, 5]])
</pre></div>
</div>
</div>
</div>
<p>Other commonly used operations include matrix multiplications, which are essential for neural networks. Quite often, we have an input vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, which is transformed using a learned weight matrix <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>. There are multiple ways and functions to perform matrix multiplication, some of which we list below:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="text-align:left head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p><code class="docutils literal notranslate"><span class="pre">torch.matmul</span></code></p></td>
<td class="text-align:left"><p>Performs the matrix product over two tensors, where the specific behavior depends on the dimensions. If both inputs are matrices (2-dimensional tensors), it performs the standard matrix product. For higher dimensional inputs, the function supports broadcasting (for details see the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=matmul#torch.matmul">documentation</a>). Can also be written as <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">&#64;</span> <span class="pre">b</span></code>, similar to Numpy.</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><code class="docutils literal notranslate"><span class="pre">torch.mm</span></code></p></td>
<td class="text-align:left"><p>Performs the matrix product over two matrices, but doesn‚Äôt support broadcasting (see <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.mm.html?highlight=torch%20mm#torch.mm">documentation</a>).</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><code class="docutils literal notranslate"><span class="pre">torch.bmm</span></code></p></td>
<td class="text-align:left"><p>Performs the matrix product with a support batch dimension. If the first tensor <span class="math notranslate nohighlight">\(\mathbf T\)</span> is of shape (<span class="math notranslate nohighlight">\(b\times n\times m\)</span>), and the second tensor <span class="math notranslate nohighlight">\(\mathbf R\)</span> (<span class="math notranslate nohighlight">\(b\times m\times p\)</span>), the output <span class="math notranslate nohighlight">\(\mathbf O\)</span> is of shape (<span class="math notranslate nohighlight">\(b\times n\times p\)</span>), and has been calculated by performing <span class="math notranslate nohighlight">\(b\)</span> matrix multiplications of the submatrices of <span class="math notranslate nohighlight">\(\mathbf T\)</span> and <span class="math notranslate nohighlight">\(\mathbf R\)</span> which can be written as <span class="math notranslate nohighlight">\(O_{ijk} = \sum_q T_{ijq} R_{iqk}.\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><code class="docutils literal notranslate"><span class="pre">torch.einsum</span></code></p></td>
<td class="text-align:left"><p>Performs matrix multiplications and more (i.e. sums of products) using the Einstein summation convention. See  <a class="reference internal" href="#ref-einstein-summation"><span class="std std-ref">below</span></a>.</p></td>
</tr>
</tbody>
</table>
<p>Usually, we use <code class="docutils literal notranslate"><span class="pre">torch.matmul</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.bmm</span></code>. We can try a matrix multiplication with <code class="docutils literal notranslate"><span class="pre">torch.matmul</span></code> below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x =&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x =
tensor([[0, 1, 2],
        [3, 4, 5]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># We can also stack multiple operations in a single line</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;w =&quot;</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>w =
tensor([[0, 1, 2],
        [3, 4, 5],
        [6, 7, 8]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="c1"># Verify the result by calculating it by hand too!</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;h =&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>h =
tensor([[15, 18, 21],
        [42, 54, 66]])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="indexing">
<h4>Indexing<a class="headerlink" href="#indexing" title="Permalink to this headline">¬∂</a></h4>
<p>We often have the situation where we need to select a part of a tensor. Indexing works just like in numpy, so let‚Äôs try it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x =&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x =
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>   <span class="c1"># Second column</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([1, 5, 9])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>      <span class="c1"># First row</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0, 1, 2, 3])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># First two rows, last column</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([3, 7])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="p">:])</span> <span class="c1"># Middle two rows</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="computational-graphs-and-backpropagation">
<h3>Computational Graphs and Backpropagation<a class="headerlink" href="#computational-graphs-and-backpropagation" title="Permalink to this headline">¬∂</a></h3>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Learnable parameters</strong></p>
</div>
<p>One of the main reasons for using PyTorch in Deep Learning projects is that we can automatically get gradients  or partial derivatives of functions that we define. We will mainly use PyTorch for implementing neural networks, and they are just fancy functions. These generally have <strong>parameters</strong> or <strong>weights</strong> whose values we want to learn.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Computational graph</strong></p>
</div>
<p>Given an input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, we define our function by a series of computations on that input, usually by matrix-multiplications with weight matrices and additions with so-called bias vectors. As we manipulate our input, we are automatically creating a <strong>computational graph</strong>. This graph shows how to arrive at our output from our input. PyTorch is a define-by-run framework; this means that we can just do our manipulations, and PyTorch will keep track of that graph for us. Thus, we create a dynamic computation graph along the way. To recap: the only thing we have to do is to compute the output, and then we can ask PyTorch to automatically get the gradients.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Gradient descent</strong></p>
</div>
<p>Why do we want gradients? Consider that we have defined a function, a neural net, that is supposed to compute a certain output <span class="math notranslate nohighlight">\(y\)</span> for an input vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. We then define an <strong>error measure</strong> that tells us how wrong our network is; how bad it is in predicting output <span class="math notranslate nohighlight">\(y\)</span> from input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Based on this error measure, we can use the gradients to update the weights <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> that were responsible for the output, so that the next time we present input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to our network, the output will be closer to what we want. The rational for the update rule is based on gradient descent, i.e. that the gradient points to the direction of greatest increase in the error function.</p>
<p>The first thing we have to do is to specify which tensors require gradients. By default, when we create a tensor, it does not require gradients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False
</pre></div>
</div>
</div>
</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Tensors = numpy arrays + GPU support + gradients</strong></p>
</div>
<p>We can change this for an existing tensor using the function <code class="docutils literal notranslate"><span class="pre">requires_grad_()</span></code> (underscore indicating that this is an in-place operation). Alternatively, when creating a tensor, you can pass the argument <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> to most initializers we have seen above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Creating a computation graph</strong></p>
</div>
<p>In order to get familiar with the concept of a computation graph, we will create one for the following function:</p>
<div class="math notranslate nohighlight">
\[y = \frac{1}{|\mathbf{x}|}\sum_i \left[(x_i + 2)^2 + 3\right]\]</div>
<p>You could imagine that <span class="math notranslate nohighlight">\(\mathbf x\)</span> are our parameters, and we want to optimize (either maximize or minimize) the output <span class="math notranslate nohighlight">\(y\)</span>. For this, we want to obtain the gradients <span class="math notranslate nohighlight">\(\partial y / \partial \mathbf{x}\)</span>. For our example, we‚Äôll use <span class="math notranslate nohighlight">\(\mathbf{x}=[0,1,2]\)</span> as our input. This value is the reference point where we take the gradient of <span class="math notranslate nohighlight">\(y\)</span>. Here <span class="math notranslate nohighlight">\(|\cdot|\)</span> is the length function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># Only float tensors can have gradients</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x =&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x = tensor([0., 1., 2.], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>Now let‚Äôs build the computation graph step by step. You can combine multiple operations in a single line, but we will separate them here to get a better understanding of how each operation is added to the computation graph.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">b</span> <span class="o">+</span> <span class="mi">3</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y =&quot;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>y = tensor(12.6667, grad_fn=&lt;MeanBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Backpropagation</strong></p>
</div>
<p>Using the statements above, we have created a computation graph that looks similar to the figure below:</p>
<center style="width: 100%"><img src="https://uvadlc-notebooks.readthedocs.io/en/latest/_images/pytorch_computation_graph.svg" width="200px"></center>
<p>We calculate <span class="math notranslate nohighlight">\(\mathbf a\)</span> based on the inputs <span class="math notranslate nohighlight">\(\mathbf x\)</span> and the constant <span class="math notranslate nohighlight">\(2\)</span>, <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> is <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> squared, and so on. The visualization is an abstraction of the dependencies between inputs and outputs of the operations we have applied.
Each node of the computation graph has automatically defined a function for calculating the gradients with respect to its inputs, <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code>. You can see this when we printed the output tensor <span class="math notranslate nohighlight">\(y\)</span>. This is why the computation graph is usually visualized in the reverse direction (arrows point from the result to the inputs). We can perform backpropagation on the computation graph by calling the function <code class="docutils literal notranslate"><span class="pre">backward()</span></code> on the last output, which effectively calculates the gradients for each tensor that has the property <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">x.grad</span></code> will now contain the gradient <span class="math notranslate nohighlight">\(\partial y/ \partial \mathbf{x}\)</span>, and this gradient indicates how a change in <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> will affect output <span class="math notranslate nohighlight">\(y\)</span> given the current input <span class="math notranslate nohighlight">\(\mathbf{x}=[0,1,2]\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([1.3333, 2.0000, 2.6667])
</pre></div>
</div>
</div>
</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Computing gradients by hand</strong></p>
</div>
<p>We can also verify these gradients by hand. We will calculate the gradients using the chain rule, in the same way as PyTorch did it. Note that each node depends on exactly the node above it in the graph. Moreover, there is no cross-index dependencies. Hence,</p>
<div class="math notranslate nohighlight">
\[\frac{\partial y}{\partial x_i} = \frac{\partial y}{\partial c_i}\frac{\partial c_i}{\partial b_i}\frac{\partial b_i}{\partial a_i}\frac{\partial a_i}{\partial x_i}\]</div>
<p>Note that we have simplified this equation to index notation, and by using the fact that all operation besides the mean do not combine the elements in the tensor. The partial derivatives are:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial a_i}{\partial x_i} = 1,\hspace{1cm}
\frac{\partial b_i}{\partial a_i} = 2\cdot a_i\hspace{1cm}
\frac{\partial c_i}{\partial b_i} = 1\hspace{1cm}
\frac{\partial y}{\partial c_i} = \frac{1}{3}
\]</div>
<p>Hence, with the input being <span class="math notranslate nohighlight">\(\mathbf{x}=[0,1,2]\)</span>, our gradients are <span class="math notranslate nohighlight">\(\partial y/\partial \mathbf{x}=[4/3,2,8/3]\)</span>. The previous code cell should have printed the same result.</p>
</div>
<div class="section" id="gpu-support">
<h3>GPU Support<a class="headerlink" href="#gpu-support" title="Permalink to this headline">¬∂</a></h3>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Introducing GPUs</strong></p>
</div>
<p>A crucial feature of PyTorch is the support of GPUs, short for Graphics Processing Unit. A GPU can perform many thousands of small operations in parallel, making it very well suitable for performing large matrix operations in neural networks. When comparing GPUs to CPUs, we can list the following main differences (credit: <a class="reference external" href="https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/">Kevin Krewell, 2009</a>)</p>
<center style="width: 100%"><img src="https://uvadlc-notebooks.readthedocs.io/en/latest/_images/comparison_CPU_GPU.png" width="700px"></center>
<br>
<p>GPUs can accelerate the training of your network up to a factor of <span class="math notranslate nohighlight">\(100\)</span> which is essential for large neural networks. PyTorch implements a lot of functionality for supporting GPUs (mostly those of NVIDIA due to the libraries <a class="reference external" href="https://developer.nvidia.com/cuda-zone">CUDA</a> and <a class="reference external" href="https://developer.nvidia.com/cudnn">cuDNN</a>). First, let‚Äôs check whether you have a GPU available:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gpu_avail</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Is the GPU available? </span><span class="si">{</span><span class="n">gpu_avail</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Is the GPU available? True
</pre></div>
</div>
</div>
</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Setting the</strong> <code class="docutils literal notranslate"><span class="pre">device</span></code> <strong>variable</strong></p>
</div>
<p>If you have a GPU on your computer but the command above returns <code class="docutils literal notranslate"><span class="pre">False</span></code>, make sure you have the correct CUDA-version installed. By default, all tensors you create are stored on the CPU. It is often a good practice to define a <code class="docutils literal notranslate"><span class="pre">device</span></code> object in your code which points to the GPU if you have one, and otherwise to the CPU. Then, you can write your code with respect to this device object, and it allows you to run the same code on both a CPU-only system, and one with a GPU. Let‚Äôs try it below. We can specify the device as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Device:&quot;</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Device: cuda
</pre></div>
</div>
</div>
</div>
<p>We can push a tensor to the GPU by using the function <code class="docutils literal notranslate"><span class="pre">.to(...)</span></code>, or <code class="docutils literal notranslate"><span class="pre">.cuda()</span></code>. Now let‚Äôs create a tensor and push it to the device:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x =&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x =
tensor([[0., 0., 0.],
        [0., 0., 0.]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
<p>In case you have a GPU, you should now see the attribute <code class="docutils literal notranslate"><span class="pre">device='cuda:0'</span></code> being printed next to your tensor. The zero next to cuda indicates that this is the zero-th GPU device on your computer. PyTorch also supports multi-GPU systems, but this you will only need once you have very big networks to train (if interested, see the <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#distributed-basics">PyTorch documentation</a>).</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>GPU speedup:</strong> matrix multiplication</p>
</div>
<p>We can also compare the runtime of a large matrix multiplication on the CPU with a operation on the GPU:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="mi">5000</span><span class="p">)</span>

<span class="c1"># CPU version</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CPU time: </span><span class="si">{</span><span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span><span class="si">:</span><span class="s2">6.5f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

<span class="c1"># GPU version</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># CUDA is asynchronous, so we need to use different timing functions</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">start</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">end</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>  <span class="c1"># Waits for everything to finish running on the GPU</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU time: </span><span class="si">{</span><span class="mf">0.001</span> <span class="o">*</span> <span class="n">start</span><span class="o">.</span><span class="n">elapsed_time</span><span class="p">(</span><span class="n">end</span><span class="p">)</span><span class="si">:</span><span class="s2">6.5f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>  <span class="c1"># Milliseconds to seconds</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU time: 1.83197s
GPU time: 0.79522s
</pre></div>
</div>
</div>
</div>
<p>Depending on the size of the operation and the CPU / GPU in your system, the speedup of this operation can be &gt;50x. As <code class="docutils literal notranslate"><span class="pre">matmul</span></code> operations are very common in neural networks, we can already see the great benefit of training a NN on a GPU. The time estimate can be relatively noisy here because we haven‚Äôt run it for multiple times. Feel free to extend this, but it also takes longer to run.</p>
<p><strong>Reproducibility of GPU operations.</strong> When generating random numbers, the seed between CPU and GPU is not synchronized. Hence, we need to set the seed on the GPU separately to ensure a reproducible code. Note that due to different GPU architectures, running the same code on different GPUs does not guarantee the same random numbers. Still, we don‚Äôt want that our code gives us a different output every time we run it on the exact same hardware. Hence, we also set the seed on the GPU:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GPU operations have a separate seed we also want to set</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span> 
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    
<span class="c1"># Additionally, some operations on a GPU are implemented stochastic for efficiency</span>
<span class="c1"># We want to ensure that all operations are deterministic on GPU (if used) for reproducibility</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">determinstic</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="learning-the-continuous-xor">
<h2>Learning the Continuous XOR<a class="headerlink" href="#learning-the-continuous-xor" title="Permalink to this headline">¬∂</a></h2>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>XOR dataset with noise</strong></p>
</div>
<p>We will introduce the libraries and all additional parts you might need to train a neural network in PyTorch, using a simple example classifier on a simple yet well known example: XOR. Given two binary inputs <span class="math notranslate nohighlight">\({x}_1\)</span> and <span class="math notranslate nohighlight">\({x}_2\)</span>, the label to predict is <span class="math notranslate nohighlight">\(1\)</span> if either <span class="math notranslate nohighlight">\({x}_1\)</span> or <span class="math notranslate nohighlight">\({x}_2\)</span> is <span class="math notranslate nohighlight">\(1\)</span> while the other is <span class="math notranslate nohighlight">\(0\)</span>, otherwise the label is <span class="math notranslate nohighlight">\(0.\)</span> The example became famous by the fact that a single neuron, i.e. a linear classifier, cannot learn this simple function. Hence, we will learn how to build a small neural network that can learn this function. To make it a little bit more interesting, we move the XOR into continuous space and introduce some gaussian noise on the binary inputs. Our desired separation of an XOR dataset could look as follows:</p>
<center style="width: 100%"><img src=https://uvadlc-notebooks.readthedocs.io/en/latest/_images/continuous_xor.svg width="350px"></center><div class="section" id="model">
<h3>Model<a class="headerlink" href="#model" title="Permalink to this headline">¬∂</a></h3>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Building neural nets</strong></p>
</div>
<p>If we want to build a neural network in PyTorch, we could specify all our parameters (weight matrices, bias vectors) using <code class="docutils literal notranslate"><span class="pre">Tensors</span></code> (with <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>), ask PyTorch to calculate the gradients and then adjust the parameters. But things can quickly get cumbersome if we have a lot of parameters. In PyTorch, there is a package called <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> that makes building neural networks more convenient.</p>
<p>The package <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> defines a series of useful classes like linear networks layers, activation functions, loss functions etc. A full list can be found <a class="reference external" href="https://pytorch.org/docs/stable/nn.html">here</a>. In case you need a certain network layer, check the documentation of the package first before writing the layer yourself as the package likely contains the code for it already. We import it below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</pre></div>
</div>
</div>
</div>
<p>Additionally to <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code>, there is also <code class="docutils literal notranslate"><span class="pre">torch.nn.functional</span></code>. It contains functions that are used in network layers. This is in contrast to <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> which defines them as <code class="docutils literal notranslate"><span class="pre">nn.Modules</span></code> (more on it below), and <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> actually uses a lot of functionalities from <code class="docutils literal notranslate"><span class="pre">torch.nn.functional</span></code>. Hence, the functional package is useful in many situations, and so we import it as well here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="nn-module">
<h4>nn.Module<a class="headerlink" href="#nn-module" title="Permalink to this headline">¬∂</a></h4>
<p>In PyTorch, a neural network is built up out of modules. Modules can contain other modules, and a neural network is considered to be a module itself as well. The basic template of a module is as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Some init for my module</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Function for performing the calculation of the module.</span>
        <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">forward</span></code> function is where the computation of the module is taken place, and is executed when you call the module (<code class="docutils literal notranslate"><span class="pre">net</span> <span class="pre">=</span> <span class="pre">MyModule();</span> <span class="pre">net(x)</span></code>). In the <code class="docutils literal notranslate"><span class="pre">init</span></code> function, we usually create the parameters of the module, using <code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code>, or defining other modules that are used in the forward function. The backward calculation is done automatically, but could be overwritten as well if wanted.</p>
</div>
<div class="section" id="simple-classifier">
<h4>Simple classifier<a class="headerlink" href="#simple-classifier" title="Permalink to this headline">¬∂</a></h4>
<p>We can now make use of the pre-defined modules in the <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> package, and define our own small neural network. We will use a minimal network with a input layer, one hidden layer with tanh as activation function, and a output layer. In other words, our networks should look something like this:</p>
<center width="100%"><img src=https://uvadlc-notebooks.readthedocs.io/en/latest/_images/small_neural_network.svg width="300px"></center>
<p>The input neurons are shown in blue, which represent the coordinates <span class="math notranslate nohighlight">\({x}_1\)</span> and <span class="math notranslate nohighlight">\({x}_2\)</span> of a data point. The hidden neurons including a tanh activation are shown in white, and the output neuron in red. The network essentially embeds each input to a subspace of <span class="math notranslate nohighlight">\(\mathbb{R}^4\)</span> and performs logistic regression on the resulting 4-dimensional representation. In PyTorch, we can implement this as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleClassifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="c1"># Initialize the modules we need to build the network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_hidden</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        
        <span class="c1"># Perform the calculation of the model to determine the prediction</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_fn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>For the examples in this notebook, we will use a tiny neural network with two input neurons and four hidden neurons. As we perform binary classification, we will use a single output neuron. Note that we do not apply a sigmoid on the output yet. This is because other functions, especially the loss, are more efficient and precise to calculate on the original outputs instead of the sigmoid output. We will discuss the detailed reason later.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleClassifier</span><span class="p">(</span><span class="n">num_inputs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_hidden</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Printing a module shows all its submodules</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>SimpleClassifier(
  (linear1): Linear(in_features=2, out_features=4, bias=True)
  (act_fn): Tanh()
  (linear2): Linear(in_features=4, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
<p>Printing the model lists all submodules it contains. The parameters of a module can be obtained by using its <code class="docutils literal notranslate"><span class="pre">parameters()</span></code> functions, or <code class="docutils literal notranslate"><span class="pre">named_parameters()</span></code> to get a name to each parameter object. For our small neural network, we have the following parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parameter </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">, shape </span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter linear1.weight, shape torch.Size([4, 2])
Parameter linear1.bias, shape torch.Size([4])
Parameter linear2.weight, shape torch.Size([1, 4])
Parameter linear2.bias, shape torch.Size([1])
</pre></div>
</div>
</div>
</div>
<p>Each linear layer has a weight matrix of the shape <code class="docutils literal notranslate"><span class="pre">[output,</span> <span class="pre">input]</span></code>, and a bias of the shape <code class="docutils literal notranslate"><span class="pre">[output]</span></code>. The tanh activation function does not have any parameters. Note that parameters are only registered for <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> objects that are direct object attributes, i.e. <code class="docutils literal notranslate"><span class="pre">self.a</span> <span class="pre">=</span> <span class="pre">...</span></code>. If you define a list of modules, the parameters of those are not registered for the outer module and can cause some issues when you try to optimize your module. There are alternatives, like <code class="docutils literal notranslate"><span class="pre">nn.ModuleList</span></code>, <code class="docutils literal notranslate"><span class="pre">nn.ModuleDict</span></code> and <code class="docutils literal notranslate"><span class="pre">nn.Sequential</span></code>, that allow you to have different data structures of modules. We will use them in a few later tutorials and explain them there.</p>
</div>
</div>
<div class="section" id="data">
<h3>Data<a class="headerlink" href="#data" title="Permalink to this headline">¬∂</a></h3>
<p>PyTorch also provides a few functionalities to load the training and test data efficiently, summarized in the package <code class="docutils literal notranslate"><span class="pre">torch.utils.data</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.utils.data</span> <span class="k">as</span> <span class="nn">data</span>
</pre></div>
</div>
</div>
</div>
<p>The data package defines two classes which are the standard interface for handling data in PyTorch: <code class="docutils literal notranslate"><span class="pre">data.Dataset</span></code>, and <code class="docutils literal notranslate"><span class="pre">data.DataLoader</span></code>. The dataset class provides an uniform interface to access the training/test data, while the data loader makes sure to efficiently load and stack the data points from the dataset into batches during training.</p>
<div class="section" id="the-dataset-class">
<h4>The <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class<a class="headerlink" href="#the-dataset-class" title="Permalink to this headline">¬∂</a></h4>
<p>The dataset class summarizes the basic functionality of a dataset in a natural way. To define a dataset in PyTorch, we simply specify two functions: (1) <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>, and (2) <code class="docutils literal notranslate"><span class="pre">__len__</span></code>. The get-item function has to return the <span class="math notranslate nohighlight">\(i\)</span>-th data point in the dataset, while the len function returns the size of the dataset. For the XOR dataset, we can define the dataset class as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">XORDataset</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Inputs:</span>
<span class="sd">            size - Number of data points we want to generate</span>
<span class="sd">            std - Standard deviation of the noise (see generate_continuous_xor function)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">std</span> <span class="o">=</span> <span class="n">std</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generate_continuous_xor</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">generate_continuous_xor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        
        <span class="c1"># Each data point in the XOR dataset has two variables, x and y, that can be either 0 or 1</span>
        <span class="c1"># The label is their XOR combination, i.e. 1 if only if x is not equal to y, otherwise zero.</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">label</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
        
        <span class="c1"># To make it slightly more challenging, we add a bit of gaussian noise to the data points.</span>
        <span class="n">data</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">std</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">label</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        
        <span class="c1"># Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
        
        <span class="c1"># Return the i-th data point of the dataset</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">label</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs try to create such a dataset and inspect it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">XORDataset</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Size of dataset:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Data point 0:&quot;</span><span class="p">,</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Size of dataset: 200
Data point 0: (tensor([0.9632, 0.1117]), tensor(1))
</pre></div>
</div>
</div>
</div>
<p>To better relate to the dataset, we visualize the samples below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">visualize_samples</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="n">data_0</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">data_1</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_0</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">data_0</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;#333&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Class 0&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_1</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">data_1</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;#333&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Class 1&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Dataset samples&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_2$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_samples</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pytorch-intro_86_0.png" src="../_images/pytorch-intro_86_0.png" />
</div>
</div>
</div>
<div class="section" id="the-dataloader-class">
<h4>The <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> class<a class="headerlink" href="#the-dataloader-class" title="Permalink to this headline">¬∂</a></h4>
<p>The class <code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> represents a Python iterable over a dataset with support for automatic batching, multi-process data loading and many more features. The data loader communicates with the dataset using the function <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>, and stacks its outputs as tensors over the first dimension to form a batch.
In contrast to the dataset class, we usually don‚Äôt have to define our own data loader class, but can just create an object of it with the dataset as input. Additionally, we can configure our data loader with the following input arguments (only a selection, see full list <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">here</a>):</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="text-align:left head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code></p></td>
<td class="text-align:left"><p>Number of samples to stack per batch.</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><code class="docutils literal notranslate"><span class="pre">shuffle</span></code></p></td>
<td class="text-align:left"><p>If True, the data is returned in a random order. This is important during training for introducing stochasticity.</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><code class="docutils literal notranslate"><span class="pre">num_workers</span></code></p></td>
<td class="text-align:left"><p>Number of subprocesses to use for data loading. The default, 0, means that the data will be loaded in the main process which can slow down training for datasets where loading a data point takes a considerable amount of time (e.g. large images). More workers are recommended for those, but can cause issues on Windows computers. For tiny datasets as ours, 0 workers are usually faster.</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><code class="docutils literal notranslate"><span class="pre">pin_memory</span></code></p></td>
<td class="text-align:left"><p>If True, the data loader will copy Tensors into CUDA pinned memory before returning them. This can save some time for large data points on GPUs. Usually a good practice to use for a training set, but not necessarily for validation and test to save memory on the GPU.</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><code class="docutils literal notranslate"><span class="pre">drop_last</span></code></p></td>
<td class="text-align:left"><p>If True, the last batch is dropped in case it is smaller than the specified batch size. This occurs when the dataset size is not a multiple of the batch size. Only potentially helpful during training to keep a consistent batch size.</p></td>
</tr>
</tbody>
</table>
<p>Let‚Äôs create a simple data loader below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># next(iter(...)) catches the first batch of the data loader</span>
<span class="c1"># If shuffle is True, this will return a different batch every time we run this cell</span>
<span class="c1"># For iterating over the whole dataset, we can simple use &quot;for batch in data_loader: ...&quot;</span>
<span class="n">data_inputs</span><span class="p">,</span> <span class="n">data_labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>

<span class="c1"># The shape of the outputs are [batch_size, d_1,...,d_N] where d_1,...,d_N are the </span>
<span class="c1"># dimensions of the data point returned from the dataset class</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Data inputs&quot;</span><span class="p">,</span> <span class="n">data_inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">data_inputs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Data labels&quot;</span><span class="p">,</span> <span class="n">data_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">data_labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data inputs torch.Size([8, 2]) 
 tensor([[-0.0890,  0.8608],
        [ 1.0905, -0.0128],
        [ 0.7967,  0.2268],
        [-0.0688,  0.0371],
        [ 0.8732, -0.2240],
        [-0.0559, -0.0282],
        [ 0.9277,  0.0978],
        [ 1.0150,  0.9689]])

Data labels torch.Size([8]) 
 tensor([1, 1, 1, 0, 1, 0, 1, 0])
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="optimization">
<h3>Optimization<a class="headerlink" href="#optimization" title="Permalink to this headline">¬∂</a></h3>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Training algorithm for NNs</strong></p>
</div>
<p>After defining the model and the dataset, it is time to prepare the optimization of the model. During training, we will perform the following steps:</p>
<ol class="simple">
<li><p>Get a batch from the data loader.</p></li>
<li><p>Obtain the predictions from the model for the batch.</p></li>
<li><p>Calculate the loss based on the difference between predictions and labels.</p></li>
<li><p>Backpropagation: calculate the gradients for every parameter with respect to the loss.</p></li>
<li><p>Update the parameters of the model in the direction of the gradients.</p></li>
</ol>
<p>We have seen how we can do step 1, 2 and 4 in PyTorch. Now, we will look at step 3 and 5.</p>
<div class="section" id="loss-modules">
<h4>Loss modules<a class="headerlink" href="#loss-modules" title="Permalink to this headline">¬∂</a></h4>
<p>We can calculate the loss for a batch by simply performing a few tensor operations as those are automatically added to the computation graph. For instance, for binary classification, we can use <strong>Binary Cross Entropy</strong> (BCE) which is defined as follows:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_\text{BCE} = -\sum_i \left[ y_i \log s_i + (1 - y_i) \log (1 - s_i) \right]\]</div>
<p>where <span class="math notranslate nohighlight">\(y\)</span> are our labels, and <span class="math notranslate nohighlight">\(s\)</span> our predictions, both in the range of <span class="math notranslate nohighlight">\([0,1]\)</span>. However, PyTorch already provides a <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#loss-functions">list of predefined loss functions</a> which we can use. For instance, for BCE, PyTorch has two modules: <code class="docutils literal notranslate"><span class="pre">nn.BCELoss()</span></code>, <code class="docutils literal notranslate"><span class="pre">nn.BCEWithLogitsLoss()</span></code>. While <code class="docutils literal notranslate"><span class="pre">nn.BCELoss</span></code> expects the inputs <span class="math notranslate nohighlight">\(s\)</span> to be in the range <span class="math notranslate nohighlight">\([0,1]\)</span>, i.e. the output of a sigmoid, <code class="docutils literal notranslate"><span class="pre">nn.BCEWithLogitsLoss</span></code> combines a sigmoid layer and the BCE loss in a single class. This version is numerically more stable than using a plain Sigmoid followed by a BCE loss because of the logarithms applied in the loss function. Hence, it is adviced to use loss functions applied on <strong>logits</strong> (i.e. pre-sigmoid scores in <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>) where possible. For our model defined above, we therefore use the module <code class="docutils literal notranslate"><span class="pre">nn.BCEWithLogitsLoss</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss_module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Remember to not apply a sigmoid on the output of the model in this case!</p>
</div>
<div class="section" id="stochastic-gradient-descent">
<h4>Stochastic Gradient Descent<a class="headerlink" href="#stochastic-gradient-descent" title="Permalink to this headline">¬∂</a></h4>
<p>For updating the parameters, PyTorch provides the package <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code> that has most popular optimizers implemented. We will discuss the specific optimizers and their differences later in the course, but will for now use the simplest of them: <code class="docutils literal notranslate"><span class="pre">torch.optim.SGD</span></code>. Stochastic Gradient Descent updates parameters by multiplying the gradients with a small constant, called <strong>learning rate</strong>, and subtracting those from the parameters (hence minimizing the loss). Therefore, we slowly move towards the direction of minimizing the loss. A good default value of the learning rate for a small network as ours is 0.1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Input to the optimizer are the parameters of the model: model.parameters()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Clearing gradients</strong></p>
</div>
<p>The optimizer provides two useful functions: <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>, and <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>. The step function updates the parameters based on the gradients as explained above. The function <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code> sets the gradients of all parameters to zero. While this function seems less relevant at first, it is a crucial pre-step before performing backpropagation. If we would call the <code class="docutils literal notranslate"><span class="pre">backward</span></code> function on the loss while the parameter gradients are non-zero from the previous batch, the new gradients would actually be <em>added</em> to the previous ones instead of overwriting them. This is done because a parameter might occur multiple times in a computation graph, and we need to sum the gradients in this case instead of replacing them. Hence, remember to call <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code> before calculating the gradients of a batch.</p>
</div>
</div>
<div class="section" id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this headline">¬∂</a></h3>
<p>Finally, we are ready to train our model. As a first step, we create a slightly larger dataset and specify a data loader with a larger batch size.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">XORDataset</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">2500</span><span class="p">)</span>
<span class="n">train_data_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we can write a small training function. Remember our five steps: load a batch, obtain the predictions, calculate the loss, backpropagate, and update. Additionally, we have to push all data and model parameters to the device of our choice (GPU if available). For the tiny neural network we have, communicating the data to the GPU actually takes much more time than we could save from running the operation on GPU. For large networks, the communication time is significantly smaller than the actual runtime making a GPU crucial in these cases. Still, to practice, we will push the data to GPU here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Push model to device. Has to be only done once</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>SimpleClassifier(
  (linear1): Linear(in_features=2, out_features=4, bias=True)
  (act_fn): Tanh()
  (linear2): Linear(in_features=4, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
<p>In addition, we set our model to <strong>training mode</strong>. This is done by calling <code class="docutils literal notranslate"><span class="pre">model.train()</span></code>. There exist certain modules that need to perform a different forward step during training than during testing (e.g. BatchNorm and Dropout), and we can switch between them using <code class="docutils literal notranslate"><span class="pre">model.train()</span></code> and <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">,</span> <span class="n">loss_module</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    
    <span class="c1"># Set model to train mode</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> 
    
    <span class="c1"># Training loop</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">data_inputs</span><span class="p">,</span> <span class="n">data_labels</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            
            <span class="c1">## Step 1: Move input data to device (only strictly necessary if we use GPU)</span>
            <span class="n">data_inputs</span> <span class="o">=</span> <span class="n">data_inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">data_labels</span> <span class="o">=</span> <span class="n">data_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            
            <span class="c1">## Step 2: Run the model on the input data</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data_inputs</span><span class="p">)</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="n">preds</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Output shape is [Batch size, 1], but we want [Batch size]</span>
            
            <span class="c1">## Step 3: Calculate the loss</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_module</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">data_labels</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
            
            <span class="c1">## Step 4: Perform backpropagation</span>
            <span class="c1"># Before calculating the gradients, we need to ensure that they are all zero. </span>
            <span class="c1"># Otherwise, gradients would not be overwritten, but added to the existing ones.</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> 
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            
            <span class="c1">## Step 5: Update the parameters</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_data_loader</span><span class="p">,</span> <span class="n">loss_module</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03&lt;00:00, 32.46it/s]
</pre></div>
</div>
</div>
</div>
<div class="section" id="saving-a-model">
<h4>Saving a model<a class="headerlink" href="#saving-a-model" title="Permalink to this headline">¬∂</a></h4>
<p>After finish training a model, we save the model to disk so that we can load the same weights at a later time. For this, we extract the so-called <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> from the model which contains all learnable parameters. For our simple model, the state dict contains the following entries:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">state_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>OrderedDict([(&#39;linear1.weight&#39;, tensor([[-2.6034, -3.3292],
        [ 1.9774, -2.4076],
        [-2.5968, -1.5908],
        [-0.5717, -0.8101]], device=&#39;cuda:0&#39;)), (&#39;linear1.bias&#39;, tensor([ 1.4459, -1.3992,  2.9882, -0.1375], device=&#39;cuda:0&#39;)), (&#39;linear2.weight&#39;, tensor([[-4.4623,  3.0885,  4.4030, -0.1377]], device=&#39;cuda:0&#39;)), (&#39;linear2.bias&#39;, tensor([-1.6853], device=&#39;cuda:0&#39;))])
</pre></div>
</div>
</div>
</div>
<p>To save the state dictionary, we can use <code class="docutils literal notranslate"><span class="pre">torch.save</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># torch.save(object, filename). For the filename, any extension can be used</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="s2">&quot;our_model.tar&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To load a model from a state dict, we use the function <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> to load the state dict from the disk, and the module function <code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code> to overwrite our parameters with the new values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load state dict from the disk (make sure it is the same name as above)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;our_model.tar&quot;</span><span class="p">)</span>

<span class="c1"># Create a new model and load the state</span>
<span class="n">new_model</span> <span class="o">=</span> <span class="n">SimpleClassifier</span><span class="p">(</span><span class="n">num_inputs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_hidden</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">new_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

<span class="c1"># Verify that the parameters are the same</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original model</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Loaded model</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">new_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original model
 OrderedDict([(&#39;linear1.weight&#39;, tensor([[-2.6034, -3.3292],
        [ 1.9774, -2.4076],
        [-2.5968, -1.5908],
        [-0.5717, -0.8101]], device=&#39;cuda:0&#39;)), (&#39;linear1.bias&#39;, tensor([ 1.4459, -1.3992,  2.9882, -0.1375], device=&#39;cuda:0&#39;)), (&#39;linear2.weight&#39;, tensor([[-4.4623,  3.0885,  4.4030, -0.1377]], device=&#39;cuda:0&#39;)), (&#39;linear2.bias&#39;, tensor([-1.6853], device=&#39;cuda:0&#39;))])

Loaded model
 OrderedDict([(&#39;linear1.weight&#39;, tensor([[-2.6034, -3.3292],
        [ 1.9774, -2.4076],
        [-2.5968, -1.5908],
        [-0.5717, -0.8101]])), (&#39;linear1.bias&#39;, tensor([ 1.4459, -1.3992,  2.9882, -0.1375])), (&#39;linear2.weight&#39;, tensor([[-4.4623,  3.0885,  4.4030, -0.1377]])), (&#39;linear2.bias&#39;, tensor([-1.6853]))])
</pre></div>
</div>
</div>
</div>
<p>A detailed tutorial on saving and loading models in PyTorch can be found <a class="reference external" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">here</a>.</p>
</div>
</div>
<div class="section" id="evaluation">
<h3>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¬∂</a></h3>
<p>Once we have trained a model, it is time to evaluate it on a held-out test set. As metric we will use accuracy since the dataset is balanced. As our dataset consist of randomly generated data points, we need to first create a test set with a corresponding data loader.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_dataset</span> <span class="o">=</span> <span class="n">XORDataset</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">test_data_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<p>When evaluating the model, we don‚Äôt need to keep track of the computation graph as we don‚Äôt intend to calculate the gradients. This reduces the required memory and speed up the model. In PyTorch, we can deactivate the computation graph using the <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">torch.no_grad():</span> <span class="pre">...</span></code> context manager. Remember to additionally set the model to <strong>eval mode</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">eval_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="c1"># Set model to eval mode</span>
    <span class="n">true_preds</span><span class="p">,</span> <span class="n">num_preds</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c1"># Context manager: deactivate gradients for the ff. code</span>
        <span class="k">for</span> <span class="n">data_inputs</span><span class="p">,</span> <span class="n">data_labels</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            
            <span class="c1"># Determine prediction of model on dev set</span>
            <span class="n">data_inputs</span><span class="p">,</span> <span class="n">data_labels</span> <span class="o">=</span> <span class="n">data_inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">data_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data_inputs</span><span class="p">)</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="n">preds</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span> <span class="c1"># Sigmoid to map predictions between 0 and 1</span>
            <span class="n">pred_labels</span> <span class="o">=</span> <span class="p">(</span><span class="n">preds</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span> <span class="c1"># Binarize predictions to 0 and 1</span>
            
            <span class="c1"># Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)</span>
            <span class="n">true_preds</span> <span class="o">+=</span> <span class="p">(</span><span class="n">pred_labels</span> <span class="o">==</span> <span class="n">data_labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="n">num_preds</span> <span class="o">+=</span> <span class="n">data_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            
    <span class="n">acc</span> <span class="o">=</span> <span class="n">true_preds</span> <span class="o">/</span> <span class="n">num_preds</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy of the model: </span><span class="si">{</span><span class="mf">100.0</span><span class="o">*</span><span class="n">acc</span><span class="si">:</span><span class="s2">4.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If we trained our model correctly, we should see a score close to 100% accuracy. However, this is only possible because of our simple task, and unfortunately, we usually don‚Äôt get such high scores on test sets of more complex tasks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eval_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_data_loader</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy of the model: 100.00%
</pre></div>
</div>
</div>
</div>
<div class="section" id="visualizing-classification-boundaries">
<h4>Visualizing classification boundaries<a class="headerlink" href="#visualizing-classification-boundaries" title="Permalink to this headline">¬∂</a></h4>
<p>To visualize what our model has learned, we can perform a prediction for every data point in a range of <span class="math notranslate nohighlight">\([-0.5, 1.5]\)</span>, and visualize the predicted class as in the sample figure at the beginning of this section. This shows where the model has created decision boundaries, and which points would be classified as <span class="math notranslate nohighlight">\(0\)</span>, and which as <span class="math notranslate nohighlight">\(1\)</span>. We therefore get a background image out of blue (class 0) and orange (class 1). The spots where the model is uncertain we will see a blurry overlap. The specific code is less relevant compared to the output figure which should hopefully show us a clear separation of classes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span> <span class="c1"># Decorator, same effect as &quot;with torch.no_grad(): ...&quot; over the whole function. (!)</span>
<span class="k">def</span> <span class="nf">visualize_classification</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)):</span>
    
    <span class="c1">## Plot data points</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="n">data_0</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">data_1</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
    
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_0</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_0</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;#333&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Class 0&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;#333&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Class 1&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Dataset samples&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_2$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    
    <span class="c1">## Plot decision function as color gradient</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">c0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">to_rgba</span><span class="p">(</span><span class="s2">&quot;C0&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">c1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">to_rgba</span><span class="p">(</span><span class="s2">&quot;C1&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>  <span class="c1"># Meshgrid function as in numpy</span>
    <span class="n">model_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">)</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
    
    <span class="n">output_image</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">preds</span><span class="p">)</span> <span class="o">*</span> <span class="n">c0</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">preds</span> <span class="o">*</span> <span class="n">c1</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>  <span class="c1"># Specifying &quot;None&quot; in a dimension creates a new one</span>
    <span class="n">output_image</span> <span class="o">=</span> <span class="n">output_image</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Convert to numpy array. This only works for tensors on CPU, hence first push to CPU</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">output_image</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="s1">&#39;lower&#39;</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">fig</span>


<span class="n">_</span> <span class="o">=</span> <span class="n">visualize_classification</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">label</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pytorch-intro_118_0.png" src="../_images/pytorch-intro_118_0.png" />
</div>
</div>
<p>The decision boundaries might not look exactly as in the figure in the preamble of this section which can be caused by running it on CPU or a different GPU architecture. Nevertheless, the result on the accuracy metric should be the approximately the same.</p>
</div>
</div>
</div>
<div class="section" id="tensorboard-logging">
<h2>TensorBoard Logging<a class="headerlink" href="#tensorboard-logging" title="Permalink to this headline">¬∂</a></h2>
<p>TensorBoard is a logging and visualization tool that is a popular choice for training deep learning models. Although initially published for TensorFlow, TensorBoard is also integrated in PyTorch allowing us to easily use it. First, let‚Äôs import it below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
<span class="o">%</span><span class="k">load_ext</span> tensorboard
</pre></div>
</div>
</div>
</div>
<p>The last line is required if you want to run TensorBoard directly in the Jupyter Notebook. Otherwise, you can start TensorBoard from the terminal.</p>
<p>PyTorch‚Äôs TensorBoard API is simple to use. We start the logging process by creating a new object, <code class="docutils literal notranslate"><span class="pre">writer</span> <span class="pre">=</span> <span class="pre">SummaryWriter(...)</span></code>, where we specify the directory in which the logging file should be saved. With this object, we can log different aspects of our model by calling functions of the style <code class="docutils literal notranslate"><span class="pre">writer.add_...</span></code>. For example, we can visualize the computation graph with the function <code class="docutils literal notranslate"><span class="pre">writer.add_graph</span></code>, or add a scalar value like the loss with <code class="docutils literal notranslate"><span class="pre">writer.add_scalar</span></code>. Let‚Äôs adapt our initial training function with adding a TensorBoard logger below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_model_with_logger</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">,</span> <span class="n">loss_module</span><span class="p">,</span> <span class="n">val_dataset</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">logging_dir</span><span class="o">=</span><span class="s1">&#39;runs/our_experiment&#39;</span><span class="p">):</span>
    <span class="c1"># Create TensorBoard logger</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="n">logging_dir</span><span class="p">)</span>
    <span class="n">model_plotted</span> <span class="o">=</span> <span class="kc">False</span>
    
    <span class="c1"># Set model to train mode</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> 
    
    <span class="c1"># Training loop</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)):</span>
        <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">data_inputs</span><span class="p">,</span> <span class="n">data_labels</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            
            <span class="c1">## Step 1: Move input data to device (only strictly necessary if we use GPU)</span>
            <span class="n">data_inputs</span> <span class="o">=</span> <span class="n">data_inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">data_labels</span> <span class="o">=</span> <span class="n">data_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            
            <span class="c1"># For the very first batch, we visualize the computation graph in TensorBoard</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">model_plotted</span><span class="p">:</span>
                <span class="n">writer</span><span class="o">.</span><span class="n">add_graph</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data_inputs</span><span class="p">)</span>
                <span class="n">model_plotted</span> <span class="o">=</span> <span class="kc">True</span>
            
            <span class="c1">## Step 2: Run the model on the input data</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data_inputs</span><span class="p">)</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="n">preds</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Output is [Batch size, 1], but we want [Batch size]</span>
            
            <span class="c1">## Step 3: Calculate the loss</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_module</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">data_labels</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
            
            <span class="c1">## Step 4: Perform backpropagation</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> 
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            
            <span class="c1">## Step 5: Update the parameters</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            
            <span class="c1">## Step 6: Take the running average of the loss</span>
            <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            
        <span class="c1"># Add average loss to TensorBoard</span>
        <span class="n">epoch_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;training_loss&#39;</span><span class="p">,</span>
                          <span class="n">epoch_loss</span><span class="p">,</span>
                          <span class="n">global_step</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Visualize prediction and add figure to TensorBoard</span>
        <span class="c1"># Since matplotlib figures can be slow in rendering, we only do it every 10th epoch</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">fig</span> <span class="o">=</span> <span class="n">visualize_classification</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">val_dataset</span><span class="o">.</span><span class="n">label</span><span class="p">)</span>
            <span class="n">writer</span><span class="o">.</span><span class="n">add_figure</span><span class="p">(</span><span class="s1">&#39;predictions&#39;</span><span class="p">,</span>
                              <span class="n">fig</span><span class="p">,</span>
                              <span class="n">global_step</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="n">writer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs use this method to train a model as before, with a new model and optimizer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleClassifier</span><span class="p">(</span><span class="n">num_inputs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_hidden</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">train_model_with_logger</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_data_loader</span><span class="p">,</span> <span class="n">loss_module</span><span class="p">,</span> <span class="n">val_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07&lt;00:00, 14.17it/s]
</pre></div>
</div>
</div>
</div>
<p>The TensorBoard file in the folder <code class="docutils literal notranslate"><span class="pre">runs/our_experiment</span></code> now contains a loss curve, the computation graph of our network, and a visualization of the learned predictions over number of epochs. To start the TensorBoard visualizer, simply run the following statement:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">tensorboard</span> --logdir runs/our_experiment
</pre></div>
</div>
</div>
</div>
<center><img src=https://uvadlc-notebooks.readthedocs.io/en/latest/_images/tensorboard_screenshot1.png width="1100px"></center>
<p>TensorBoard visualizations can help to identify possible issues with your model, and identify situations such as overfitting. You can also track the training progress while a model is training, since the logger automatically writes everything added to it to the logging file. Feel free to explore the TensorBoard functionalities, and we will make use of TensorBoards a couple of times from Tutorial 5 on.</p>
</div>
<div class="section" id="appendix-einstein-summation">
<span id="ref-einstein-summation"></span><h2>Appendix: Einstein summation<a class="headerlink" href="#appendix-einstein-summation" title="Permalink to this headline">¬∂</a></h2>
<p><b>Rules</b></p>
<ol class="simple">
<li><p>Repeated indices are summed over.</p></li>
<li><p>Omitting an index means that axis will be summed.</p></li>
<li><p>Unsummed axes can be returned in any order.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Summing all entries of a tensor from Rule 2 -- einsum also implemented in numpy!</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;i -&gt;&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;i -&gt;&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.0
tensor(3.)
</pre></div>
</div>
</div>
</div>
<p>Einstein summation lets us perform sums over products without having to do reshape gymnastics:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>
<span class="p">])</span>

<span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">])</span>

<span class="c1"># Matrix-matrix multiplication</span>
<span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ij, jk -&gt; ik&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[17,  4],
        [47, 10]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Matrix-vector multiplication</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ij, j -&gt; i&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 6, 15])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sum all rows, i.e. we are left with column index</span>
<span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ij -&gt; j&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([5, 7, 9])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hadamard product</span>
<span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ij, ij -&gt; ij&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 1,  4,  9],
        [16, 25, 36]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Outer product</span>
<span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;i, j -&gt; ij&#39;</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get diagonal elements of a matrix</span>
<span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ii -&gt; i&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([1., 1., 1.])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute trace</span>
<span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ii -&gt;&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(3.)
</pre></div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="seb3/tensorflow-mechanics.html" title="previous page">Mechanics of TensorFlow</a>
    <a class='right-next' id="next-link" href="pytorch-activation.html" title="next page">Activation Functions</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By ùó•ùóºùóª ùó†ùó≤ùó±ùó∂ùóªùóÆ. Powered by <a href="https://jupyterbook.org">Jupyter Book</a>.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>