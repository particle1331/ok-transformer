{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Tracking and Model Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=brightgreen)\n",
    "[![Source](https://img.shields.io/static/v1.svg?label=GitHub&message=Source&color=181717&logo=GitHub)](https://github.com/particle1331/ok-transformer/blob/master/docs/nb/mlops/02-mlflow)\n",
    "[![Stars](https://img.shields.io/github/stars/particle1331/ok-transformer?style=social)](https://github.com/particle1331/ok-transformer)\n",
    "\n",
    "```text\n",
    "ùóîùòÅùòÅùóøùó∂ùóØùòÇùòÅùó∂ùóºùóª: Notes for Module 2 of the MLOps Zoomcamp (2022) by DataTalks.Club.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, we will look **experiment tracking** and **model management**. A machine learning experiment is defined as a session or process of making machine learning models. Experiment tracking is the process of keeping track of all relevant information in an experiments. This includes source code, environment, data, models, hyperparameters, and so on which are important for reproducing the experiment as well as for making actual predictions. \n",
    "\n",
    "Manual tracking, e.g. with spreadsheets, is error prone, not standardized, has low visibility, and difficult for teams to collaborate over. As an alternative, we will use the experiment tracking platform [**MLflow**](https://mlflow.org/). As we have seen with prototyping a ride duration model, having the ability to reproduce results is important since we want to have the same results when deploying the model in different environments. Using experiment tracking and model management platforms allows us to have better chance at reproducing our results, as well as aid in model management and optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started: MLflow UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the MLflow UI with an SQLite backend as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "$ mlflow ui --backend-store-uri sqlite:///mlflow.db\n",
    "\n",
    "[2022-05-26 19:35:22 +0800] [92498] [INFO] Starting gunicorn 20.1.0\n",
    "[2022-05-26 19:35:22 +0800] [92498] [INFO] Listening at: http://127.0.0.1:5000 (92498)\n",
    "[2022-05-26 19:35:22 +0800] [92498] [INFO] Using worker: sync\n",
    "[2022-05-26 19:35:22 +0800] [92499] [INFO] Booting worker with pid: 92499\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our experiment, we will use our code and data from [Module 1](https://particle1331.github.io/ok-transformer/nb/mlops/1-intro.html). So before doing any run, we either create an **experiment** or connect a run to it if the experiment already exists. This also sets the experiment tracking backend. The same one that is visualized in the UI above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`experiment_lr.py`](https://github.com/particle1331/ok-transformer/blob/mlops/docs/nb/mlops/02-mlflow/experiment_lr.py#L25-L27)\n",
    "```\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section of the code executes a **single run** of the experiment. Note the logging at the end of the script. This is quite involved, but here we want to show what types of data can be logged in MLflow. Everything that runs inside the following context is a single run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`experiment_lr.py`](https://github.com/particle1331/ok-transformer/blob/mlops/docs/nb/mlops/02-mlflow/experiment_lr.py#L30-L62)\n",
    "```\n",
    "```python\n",
    "with mlflow.start_run():\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # MLflow logging\n",
    "    start_time = time.time()\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_valid = model.predict(X_valid)\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    rmse_train = mean_squared_error(y_train, y_pred_train, squared=False)\n",
    "    rmse_valid = mean_squared_error(y_valid, y_pred_valid, squared=False)\n",
    "\n",
    "    fig = plot_duration_distribution(model, X_train, y_train, X_valid, y_valid)\n",
    "    fig.savefig(artifacts / 'plot.svg')\n",
    "\n",
    "    mlflow.set_tag('author', 'particle')\n",
    "    mlflow.set_tag('model', 'baseline')\n",
    "    \n",
    "    mlflow.log_param('train_data_path', train_data_path)\n",
    "    mlflow.log_param('valid_data_path', valid_data_path)\n",
    "    \n",
    "    mlflow.log_metric('rmse_train', rmse_train)\n",
    "    mlflow.log_metric('rmse_valid', rmse_valid)\n",
    "    mlflow.log_metric(\n",
    "        'inference_time', \n",
    "        inference_time / (len(y_pred_train) + len(y_pred_valid))\n",
    "    )\n",
    "    \n",
    "    mlflow.log_artifact(artifacts / 'plot.svg')\n",
    "    mlflow.log_artifact(artifacts / 'preprocessor.pkl', artifact_path='preprocessing')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this script, we see in the UI as a single run under the `nyc-tax-experiment` experiment. MLflow is able to obtain the version from `git` and the user from the system, i.e. the user that is currently logged in. The other values are obtained from the logs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/single-run-mlflow.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we click on the run, we can see all details about it that were logged. Shown here are the date of the run, the user that executed it, total runtime, the source code used, as well as the git commit hash. Hence, it is best practice to always commit before running experiments. This ties your runs with a specific version of the code. Status `FINISHED` indicates that the script successfully ran. These are useful metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/mlflow-single-run.png\n",
    "---\n",
    "---\n",
    "Logged run of the baseline model. MLflow allows us to preview saved artifacts. Shown here is a plot of distribution of true and predicted target values.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the details of the trained model, we have parameters which here include only the path of the dataset for training and validation (only paths, no versioning). Most importantly, we can see the logged RMSEs `5.7` (train) and `7.759` (valid). The plot of the distributions of the true and predicted distributions which we logged as a training artifact is also conveniently displayed here. Finally, we log the preprocessor as **artifact** which will be needed for preprocessing test data during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we go deeper into experiment tracking with MLflow. We show how to iterate over different models and different parameters. This really just involves wrapping the run function in a loop. The nontrivial part is how to construct the sequence of parameters to loop over. For this we use [Hyperopt](https://hyperopt.github.io/hyperopt/) which implements the [TPE algorithm](https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf). \n",
    "\n",
    "We also look at the **autologging** feature for supported frameworks. This automates logging of framework specific values such as intermediate values for models trained incrementally such as XGBoost or neural networks, or the estimator class for scikit-learn models. But more importantly, autologging generates the `MLModel` file which makes it easy to deploy models for downstream tasks. Really convenient feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scikit-learn models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate different models, we will define a `run` function that takes in parameters that define and configure a run of the experiment. Then, we define a `main` function that controls the runs that will be executed. Here the `model_class` parameter controls which scikit-learn model is used to model the dataset. Note that this connects to the same tracking URI and same experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`experiment_sklearn.py`](https://github.com/particle1331/ok-transformer/blob/mlops/docs/nb/mlops/02-mlflow/experiment_sklearn.py)\n",
    "```\n",
    "```python\n",
    "def setup():\n",
    "    \n",
    "    # Preprocessing\n",
    "    ...\n",
    "\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "\n",
    "def run(model_class):\n",
    "    with mlflow.start_run():\n",
    "\n",
    "        model = model_class()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # MLflow logging\n",
    "        ...\n",
    "\n",
    "\n",
    "def main():\n",
    "    for model_class in [\n",
    "        Ridge,\n",
    "        RandomForestRegressor, \n",
    "        GradientBoostingRegressor,\n",
    "    ]:\n",
    "        run(model_class)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup()\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GradientBoostingRegressor` has the best validation score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/sklearn-results.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that autologging is available for scikit-learn models. Using this, parameters (even default ones) are automatically logged. Also, this generates the `MLModel` file along with the environments files and the serialized model. For scikit-learn models we can also see the `estimator_class` of the model. Finally, the input and output signature of the model is also stored. Autologging for supported frameworks can be activated by calling `mlflow.<framework>.autolog()`. This is a really convenient feature of MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/autolog-sk1.png\n",
    "```\n",
    "\n",
    "```{figure} ../../../img/autolog-sk2.png\n",
    "---\n",
    "---\n",
    "Autologging of a Random Forest model in scikit-learn.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization (XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous runs, `GradientBoostingRegressor` has the best performance on this task. So we try out XGBoost next. Moreover, we find the best parameter of XGBoost using Hyperopt. Here the parameters are sampled using the TPE algorithm over the search space defined in the `search_space` dictionary. We look at functionalities MLflow provides to assist with hyperparameter optimization.\n",
    "\n",
    "As before, we will define a `setup` function which sets up the required datasets and connections, define a run function (here named `objective`), and a `main` function to facilitate the runs. Finally, we define command line arguments, so we can easily control the details of the runs in the command line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`experiment_xgboost.py`](https://github.com/particle1331/ok-transformer/blob/mlops/docs/nb/mlops/02-mlflow/experiment_xgboost.py)\n",
    "```\n",
    "```python\n",
    "def setup():\n",
    "\n",
    "    # Preprocessing\n",
    "    ... \n",
    "    \n",
    "    # Set experiment\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "    mlflow.xgboost.autolog()\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Compute validation RMSE (one trial = one run).\"\"\"\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        model = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=xgb_train,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(xgb_valid, 'validation')],\n",
    "            early_stopping_rounds=50\n",
    "        )\n",
    "\n",
    "        # MLflow logging\n",
    "        ...\n",
    "    \n",
    "    return {'loss': rmse_valid, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "    'objective': 'reg:squarederror',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "\n",
    "def main(num_runs):\n",
    "    best_result = fmin(\n",
    "        fn=partial(objective),\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=num_runs,\n",
    "        trials=Trials()\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--num_runs\", default=1, type=int)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Experiment runs\n",
    "    setup()\n",
    "    main(num_runs=args.num_runs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the details of a single XGBoost run with autologging. Here all 14 parameters of XGBoost are logged (much more than what we have in our search space). Also, we have feature importances and the `MLModel` file along with the environments files. Moreover, we get metrics that are relevant for models trained incrementally, such as the best and stopped iterations (early stopped), as well as a plot of the intermediate values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/xgboost-run.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/xgboost-intermediate.png\n",
    "---\n",
    "---\n",
    "Intermediate values plot for an autologged XGBoost model.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We executed 30 runs of the XGBoost model (+1 with autologging). This can be analyzed using the compare button after selecting the results with the search query `tags.model = 'xgboost'`. Below we analyze the runs with the parallel coordinate plot. First, we have the specify the parameters and the metric. Then, we can use filters to filter out ranges that result in low objective values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/xgboost-parallel.png\n",
    "```\n",
    "\n",
    "In the other tabs, we also have scatter and contour interactive [plotly](https://plotly.com/) plots:\n",
    "\n",
    "```{figure} ../../../img/xgboost-scatter.png\n",
    "```\n",
    "\n",
    "```{figure} ../../../img/xgboost-contour.png\n",
    "---\n",
    "---\n",
    "Plots for analyzing the hyperparameter search space of an XGBoost model on this dataset.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we take a deeper look at **model management**. In addition to experiment tracking, part of model management is to do model versioning and model deployment. Model management using file systems, typically involving only a file name and date modified, is error prone. There is no versioning and no [model lineage](https://aws.amazon.com/blogs/machine-learning/model-and-data-lineage-in-machine-learning-experimentation/). Model lineage refers to all associations between a model and all components involved in its creation.\n",
    "Having no model lineage makes it difficult to track results and progress, not to mention reproducing them. \n",
    "\n",
    "For each run a `run_id` key is assigned which maps to information such as metrics, source version, and other metadata in the database. If we make sure to commit the code before running model training, then we can trace the source for each run. Each run also has an `artifacts` directory which saves in disk all logged artifacts. \n",
    "\n",
    "Autologging also generates a [`MLModel` file](https://www.mlflow.org/docs/latest/models.html) which provides a standard format for packaging models that can be used in a variety of downstream tools (e.g. serving through a REST API or batch inference on Apache Spark). This includes package dependencies and Python version used for training, as well as the input and output signature of the model. \n",
    "\n",
    "This should allow you to reproduce results of experiment runs with relative ease. Indeed, below we perform inference using the stored preprocessing pipeline and load the model from the `MLModel` file. Note that the full path can be obtained by clicking on the copy button. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/autologging-1.png\n",
    "---\n",
    "---\n",
    "In addition to run metadata, the `MLModel` file obtained with autologging preserves model lineage.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model file loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we try to perform inference on test data using the logged model and artifacts. Note that new data has no labels when processed for inference. Let us try to reproduce the valid RMSE of `6.656`. Recall that the model was validated only for rides between 1 and 60 minutes in duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.655558028101635\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "import joblib\n",
    "from utils import runs, data_path, compute_targets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Configure paths to run\n",
    "run_path = runs / '1' / 'f46f9fedb22c4411b6e265f8e65edbe3'\n",
    "model_path = run_path / 'artifacts' / 'model'\n",
    "model_artifacts_path = run_path / 'artifacts' / 'preprocessing'\n",
    "valid_data_path = data_path / 'green_tripdata_2021-02.parquet'\n",
    "\n",
    "# Preprocessing test data\n",
    "feature_pipe = joblib.load(model_artifacts_path / 'preprocessor.pkl')\n",
    "X = pd.read_parquet(valid_data_path)\n",
    "y = compute_targets(X)\n",
    "\n",
    "X = X[(y >= 1) & (y <= 60)]\n",
    "y = y[(y >= 1) & (y <= 60)]\n",
    "X = feature_pipe.transform(X)\n",
    "\n",
    "# Inference using MLModel file\n",
    "loaded_model = mlflow.pyfunc.load_model(model_path)\n",
    "\n",
    "# Reproducing metric: expected 6.656\n",
    "print(mean_squared_error(y, loaded_model.predict(X), squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the scenario where a data scientist member of our team chooses a new model for production. As deployment engineers, we naturally have the following questions in mind: What has changed in this new model? Is there any preprocessing needed? What are the dependencies? Without experiment tracking, this requires a lot of back and fort communication with the data scientist. If there is an incident in production and we had to rollback the model version, we have to go back to the email thread to determine what changed in this new version. Moreover, it might not be possible to get back the previous model (information of how it was trained has been lost). \n",
    "\n",
    "Recall that having a experiment tracking database and standard model files, solves most of the problems above. Having a **model registry** takes care of the last details of versioning and staging. For example, if we want to rollback models, we can only look at the archived models. Note that the model registry does not perform actual deployment of models, it only serves as standardization the process of moving models to production. Further integration with a CI/CD pipeline is needed if we want to trigger deployment steps along with the change in registry labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[Source: `Databricks`](https://databricks.com/blog/2020/04/15/databricks-extends-mlflow-model-registry-with-enterprise-features.html)\n",
    "```\n",
    "```{figure} ../../../img/model-registry-diagram.png\n",
    "---\n",
    "width: 35em\n",
    "---\n",
    "Having a model registry allows separation of concern. The data scientist only decides which models are ready for production while the deployment engineer takes care of moving models from staging to production, from production to archived.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the models, first we filter with `metrics.rmse_valid < 6.4`. Then, we can look at the tradeoff between valid RMSE with inference time. The scatter plot is the best tool for this. \n",
    "The tradeoffs should be weighed against production requirements. Next step is to analyze the subsets of the data where these top models are wrong, perhaps create an ensemble of them, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/xgboost-select.png\n",
    "```\n",
    "\n",
    "```{figure} ../../../img/xgboost-tradeoff.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot, we look at the 5 points within `20e-6` inference time and `6.32` valid RMSE. For the sake of demonstration, these will make up different versions of our model. The rightmost model in the scatter plot is version 1 of ride duration prediction model. To register this, we simply click the `Register Model` button in the UI. For our model name, we choose `nyc-ride-duration-model` and we stage this is production. The model registry can be viewed in the `Models` tab of the MLflow UI. We can register any other model to update the existing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/register-model.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/staging.png\n",
    "---\n",
    "---\n",
    "Registering a model and staging a new version of the model trained on the same task.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/production.png\n",
    "---\n",
    "---\n",
    "Transitioning the staged model to production and the current model to archived.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we look at how interact with the MLflow through an API. The idea is that everything that we can do in the UI by clicking buttons, we should be able to do here in code. Also, the fields that are available when using the UI correspond to the function arguments of the corresponding API endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MlflowClient` connects to the experiment tracking server. This provides a [CRUD interface](https://en.wikipedia.org/wiki/Create,_read,_update_and_delete) for managing experiments and runs. To demonstrate, we create an experiment using the client and then fetch this experiment by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experiment)\n",
      "    experiment_id=0\n",
      "    name='Default'\n",
      "    artifact_location='./mlruns/0'\n",
      "\n",
      "(Experiment)\n",
      "    experiment_id=1\n",
      "    name='nyc-taxi-experiment'\n",
      "    artifact_location='./mlruns/1'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
    "client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\n",
    "\n",
    "\n",
    "def print_experiment(experiment):\n",
    "    print(f\"(Experiment)\")\n",
    "    print(f\"    experiment_id={experiment.experiment_id}\")\n",
    "    print(f\"    name='{experiment.name}'\")\n",
    "    print(f\"    artifact_location='{experiment.artifact_location}'\")\n",
    "    print()\n",
    "\n",
    "\n",
    "for experiment in client.list_experiments():\n",
    "    print_experiment(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experiment)\n",
      "    experiment_id=7\n",
      "    name='test-create-experiment'\n",
      "    artifact_location='./mlruns/7'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client.create_experiment(name='test-create-experiment')\n",
    "print_experiment(client.get_experiment_by_name('test-create-experiment'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we filtered out the five best runs using the UI. This can be done with the client using the `search_runs` method. Note that MLflow stores even deleted experiments. So we specify `ViewType` to `ACTIVE_ONLY` to include only runs that are active (i.e. not deleted) in the search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 1e5a211d4dd84158ade0f3131a6b467a   rmse_valid: 6.286   inference_time: 1.6639e-05\n",
      "run_id: a5c39ab3d68d4de791529d083edab919   rmse_valid: 6.293   inference_time: 1.1944e-05\n",
      "run_id: f167aef23eef4305b798771a76980bd3   rmse_valid: 6.294   inference_time: 5.9997e-06\n",
      "run_id: 167836ec92094ac2b395387619e0a3b8   rmse_valid: 6.295   inference_time: 1.4946e-05\n",
      "run_id: bc2c191810a646f287c428126b75f6eb   rmse_valid: 6.307   inference_time: 1.3798e-05\n"
     ]
    }
   ],
   "source": [
    "from mlflow.entities import ViewType\n",
    "\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=1,\n",
    "    filter_string='metrics.rmse_valid < 6.32 and metrics.inference_time < 20e-6',\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    max_results=5,\n",
    "    order_by=[\"metrics.rmse_valid ASC\"]\n",
    ")\n",
    "\n",
    "for run in runs:\n",
    "    print(f\"run_id: {run.info.run_id}   rmse_valid: {run.data.metrics['rmse_valid']:.3f}   inference_time: {run.data.metrics['inference_time']:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code puts a model into the registry. If the model name already exists, then we get a new version. Otherwise it creates a new model in the model registry. Below we create a new version of our model with almost the same error rate and half the inference time of the current model in production. Observe that MLflow likes to work with the latest version for each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'nyc-ride-duration-model' already exists. Creating a new version of this model...\n",
      "2022/05/31 20:11:42 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: nyc-ride-duration-model, version 3\n",
      "Created version '3' of model 'nyc-ride-duration-model'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "run_id = 'f167aef23eef4305b798771a76980bd3'\n",
    "\n",
    "mlflow.register_model(\n",
    "    model_uri=f\"runs:/{run_id}/model\", \n",
    "    name='nyc-ride-duration-model'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version can be transitioned to staging as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def transition_stage(client, model_name, model_version, new_stage, archive_existing=False):\n",
    "    \"\"\"Transition version stage. Update description.\"\"\"\n",
    "\n",
    "    old_description = client.get_model_version(name=model_name, version=model_version).description or ''\n",
    "\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=model_version,\n",
    "        stage=new_stage,\n",
    "        archive_existing_versions=archive_existing\n",
    "    )\n",
    "\n",
    "    client.update_model_version(\n",
    "        name=model_name,\n",
    "        version=model_version,\n",
    "        description=f\"[{datetime.datetime.now()}] The model version {model_version} was transitioned to {new_stage}.\\n{old_description}\"\n",
    "    )\n",
    "\n",
    "\n",
    "transition_stage(client, 'nyc-ride-duration-model', 3, \"Staging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/update-version.png\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lists the latest versions for each stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 1   Run ID: bc2c191810a646f287c428126b75f6eb   Stage: Archived\n",
      "Version: 2   Run ID: 167836ec92094ac2b395387619e0a3b8   Stage: Production\n",
      "Version: 3   Run ID: f167aef23eef4305b798771a76980bd3   Stage: Staging\n"
     ]
    }
   ],
   "source": [
    "latest_versions = client.get_latest_versions(name='nyc-ride-duration-model')\n",
    "for version in latest_versions:\n",
    "    print(f\"Version: {version.version}   Run ID: {version.run_id}   Stage: {version.current_stage}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same transition function can be used to promote Version 3 to production and archive Version 2 which is currently in production. Note that we can automatically archive Version 2. But we use the transition function so that the description is automatically updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_stage(client, 'nyc-ride-duration-model', 3, \"Production\")\n",
    "transition_stage(client, 'nyc-ride-duration-model', 2, \"Archived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/final-versions.png\n",
    "```\n",
    "\n",
    "```{figure} ../../../img/version3-prod.png\n",
    "```\n",
    "\n",
    "```{figure} ../../../img/version2-archived.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with staged models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make inference using models in specific stages. Here we use the production model to perform inference. This can be useful if we want to compare a production model to new models in staging. Or if we want to monitor the performance of the production model on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 ¬µs, sys: 0 ns, total: 1 ¬µs\n",
      "Wall time: 1.91 ¬µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.293886787130318"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import runs, filter_target_outliers\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Prepare datasets for testing\n",
    "valid_data_path = data_path / 'green_tripdata_2021-02.parquet'\n",
    "\n",
    "X = pd.read_parquet(valid_data_path)\n",
    "y = compute_targets(X)\n",
    "X, y = filter_target_outliers(X, y)\n",
    "\n",
    "# Fix paths\n",
    "model_name = 'nyc-ride-duration-model'\n",
    "stage = \"Production\"\n",
    "run_id = client.get_latest_versions(name=model_name, stages=[stage])[0].run_id\n",
    "artifacts_path = client.download_artifacts(run_id=run_id, path='preprocessing')\n",
    "\n",
    "# Fetching the preprocessor and staged model (loads latest)\n",
    "preprocessor = joblib.load(f\"{artifacts_path}/preprocessor.pkl\")\n",
    "prod_model = mlflow.pyfunc.load_model(f\"models:/{model_name}/{stage}\")\n",
    "\n",
    "%time\n",
    "mean_squared_error(y, prod_model.predict(preprocessor.transform(X)), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark.** MLflow seems to like working only with latest versions in the registry. For example, the line `mlflow.pyfunc.load_model(f\"models:/{model_name}/{stage}\")` above silently loads the latest model at the given stage. Typically, you would like to have only one model in the production stage, but it could be useful to have multiple models in staging. A workaround would be to retrieve the model by using the model version, i.e. use `f\"models:/{model_name}/{model_version}\"` as `model_uri`. Or fetch all versions of a registered model using:\n",
    "\n",
    "```python\n",
    "model_versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote tracking server and artifact store on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For storing runs and artifacts, MLflow uses two components for storage: **backend store** and **artifact store**. The backend store persists MLflow entities such as runs, parameters, metrics, tags, notes, metadata. The artifact store persists artifacts such as files, models, images, in-memory objects. In our previous local setup, MLflow runs are recorded in a local SQLite database, and artifacts persisted as local files in our file system. But the backend can be set to any SQLAlchemy compatible database, and MLflow artifacts can be persisted a variety of remote file storage solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we configure MLflow with a remote backend and artifacts store, and a remote tracking server in AWS. This setup is useful for a multiple data scientists in a team working on multiple ML models. Here we will configure: 1) an S3 bucket as artifacts store, 2) a PostgreSQL database in RDS as backend store, and 3) an EC2 instance for running the remote tracking server. The experiments can then be explored by accessing the remote server.\n",
    "\n",
    "```{margin}\n",
    "[`Scenario 4`](https://www.mlflow.org/docs/latest/tracking.html#scenario-4-mlflow-with-remote-tracking-server-backend-and-artifact-stores)\n",
    "```\n",
    "```{figure} ../../../img/mlflow-scenario-4.png\n",
    "---\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EC2 Instance\n",
    "\n",
    "**Launch instance.** Start by logging to your IAM account. Here we launch a `t2.micro` EC2 instance with name `mlflow-tracking-server` and with `Amazon Linux 2 AMI (HVM))` 64-bit (x86) OS. These options are both free tier eligible. Create a new [key pair](https://particle1331.github.io/ok-transformer/nb/mlops/1-intro.html#renting-an-ec2-instance) so you can connect using SSH. Keep the default values for the other settings. Review the summary and launch the instance.\n",
    "\n",
    "**Inbound rules.** Go to security groups and edit inbound rules. Our instance should accept incoming SSH (port 22) and HTTP connections (port 5000). Specify CIDR blocks to specify the range of IP addresses that has access to the tracking server, e.g. choose 'My IP' as source so only computers on your local network can access the server.\n",
    "\n",
    "<br>\n",
    "\n",
    "```{figure} ../../../img/inbound-rules.png\n",
    "---\n",
    "name: ec2-inbound\n",
    "---\n",
    "Choosing `0.0.0.0/0` allows all incoming HTTP access.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Bucket\n",
    "\n",
    "**Create bucket.** Next we create the artifacts storage. Go to S3 and click on 'Create bucket'. Fill in the bucket name with something that is unique in the all regions. In our case, we found `mlflow-artifacts-store-2` to be unique. Leave all the other configurations with their default values. That's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PostgreSQL DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create database.** Go to the RDS Console and click on 'Create database'. Make sure to select PostgreSQL engine type and the Free tier template. Select an identifier for your DB instance, e.g. `mlflow-backend-db` shown here. Set the master username as `mlflow`. Tick the option 'Auto generate a password' so Amazon RDS generate a password automatically. Finally, on the section 'Additional configuration' specify an initial database name, e.g. `mlflow_backend_db`, so RDS automatically creates an initial database for you. The generated password will be shown (only once) after the database has been created. You can use the default values for all the other configurations.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "```{figure} ../../../img/rds-template.png\n",
    "---\n",
    "width: 40em\n",
    "---\n",
    "Choosing an engine and template.\n",
    "```\n",
    "\n",
    "```{figure} ../../../img/rds-identifier.png\n",
    "---\n",
    "width: 40em\n",
    "---\n",
    "\n",
    "```\n",
    "\n",
    "```{figure} ../../../img/rds-name.png\n",
    "---\n",
    "width: 40em\n",
    "---\n",
    "Choosing identifier, username, password, and initial database name. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credentials.** Take note of the following information: master username (`DB_USERNAME`), password (`DB_PASSWORD`), initial database name (`DB_NAME`), and endpoint (`DB_ENDPOINT`). On the RDS dashboard, click on the database and look at 'Connectivity & security'. The endpoint for the database can be seen there. \n",
    "\n",
    "**Inbound rules.** Select the VPC security group of the DB under the same tab. Click the security group ID, and edit inbound rules by adding a new rule that allows PostgreSQL connections on the port 5432 from the security group of the EC2 instance for the tracking server. Note that the security group of the EC2 instance is `sg-049fa46c4cd030db2 - launch-wizard-2` as can be seen on the top of {numref}`ec2-inbound`. This way, the tracking server will be able to connect to the PostgreSQL database. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/postgres.png\n",
    "---\n",
    "width: 40em\n",
    "---\n",
    "\n",
    "Allow postgres connections from the tracking server to the backend database.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server config and launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we connect to the `mlflow-tracking-server` EC2 instance from our local terminal. Change to the local `.ssh` directory then:\n",
    "\n",
    "```bash\n",
    "chmod 400 mlflow-tacking.pem\n",
    "ssh -i \"mlflow-tracking.pem\" ec2-user@ec2-34-209-62-152.us-west-2.compute.amazonaws.com\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following commands to install the dependencies, configure the environment on the EC2 instance:\n",
    "\n",
    "```\n",
    "sudo yum update\n",
    "pip3 install mlflow boto3 psycopg2-binary\n",
    "aws configure   # Input your IAM credentials here\n",
    "aws s3 ls       # Test if instance can access S3 bucket\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, launch the server by replacing the following with the appropriate database credentials. You can replace the variables below manually, or you can execute the following assignments for each variable, so that the values are automatically filled.\n",
    "\n",
    "```bash\n",
    "export DB_USER=mlflow\n",
    "export DB_PASSWORD=ZbTddA0Zc8LxYcdLFUQr\n",
    "export DB_ENDPOINT=mlflow-backend-database.csegt7oxppl.us-west-2.rds.amazonaws.com\n",
    "export DB_NAME=mlflow_backend_database\n",
    "export S3_BUCKET_NAME=mlflow-artifact-store-2\n",
    "\n",
    "mlflow server -h 0.0.0.0 -p 5000 \\\n",
    "    --backend-store-uri=postgresql://${DB_USER}:${DB_PASSWORD}@${DB_ENDPOINT}:5432/${DB_NAME} \\\n",
    "    --default-artifact-root=s3://${S3_BUCKET_NAME}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are allowed by the inbound rules to send information via HTTP to the tracking server, we should be able to send requests to it. Note that everything that follows is done in our local Jupyter notebook. After setting the tracking URI we should be able to manage experiments and the model registry through the client as before. Note that you may have to do `aws configure` for credentials in an external machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "\n",
    "TRACKING_SERVER_HOST = \"ec2-34-209-62-152.us-west-2.compute.amazonaws.com\"\n",
    "mlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:5000\")\n",
    "\n",
    "client = MlflowClient(tracking_uri=f\"http://{TRACKING_SERVER_HOST}:5000\")\n",
    "print(client.list_experiments())\n",
    "print(client.list_registered_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running an example experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/06/04 03:44:56 INFO mlflow.tracking.fluent: Experiment with name 'iris' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "mlflow.set_experiment(\"iris\")\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "for C in [10, 1, 0.1, 0.01, 0.001]:\n",
    "    with mlflow.start_run(nested=True):\n",
    "\n",
    "        params = {\"C\": C, \"random_state\": 42}\n",
    "        \n",
    "        lr = LogisticRegression(**params).fit(X, y)\n",
    "        y_pred = lr.predict(X)\n",
    "\n",
    "        mlflow.log_metric(\"accuracy\", accuracy_score(y, y_pred))\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.sklearn.log_model(lr, artifact_path=\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "```{figure} ../../../img/aws-runs.png\n",
    "---\n",
    "width: 40em\n",
    "---\n",
    "\n",
    "Runs are recorded in the tracking UI.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```{figure} ../../../img/s3.png\n",
    "---\n",
    "width: 40em\n",
    "---\n",
    "\n",
    "We can see the runs artifacts stored in the S3 bucket.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Utility code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the experiment run scripts, we used the following utility script to process the datasets used for training. Here we define a preprocessing pipeline `feature_pipe` which allows saving a single preprocessor file as artifact. This also keeps the code clean (i.e. the `transform` method is called only on the raw datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from toolz import compose\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "from matplotlib_inline import backend_inline\n",
    "backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "\n",
    "# Config variables\n",
    "root = Path(__file__).parent.resolve()\n",
    "artifacts = root / 'artifacts'\n",
    "artifacts.mkdir(exist_ok=True)\n",
    "runs = root / 'mlruns'\n",
    "data_path = root / 'data'\n",
    "\n",
    "\n",
    "class PrepareFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Prepare features for dict vectorizer.\"\"\"\n",
    "\n",
    "    def __init__(self, categorical, numerical):\n",
    "        self.categorical = categorical\n",
    "        self.numerical = numerical\n",
    "        self.features = categorical + numerical\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X['PU_DO'] = X['PULocationID'].astype(str) + '_' + X['DOLocationID'].astype(str)\n",
    "        X[self.categorical] = X[self.categorical].astype(str)\n",
    "        X = X[self.features].to_dict(orient='records')\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "def compute_targets(data):\n",
    "    \"\"\"Derive target from pickup and dropoff datetimes.\"\"\"\n",
    "\n",
    "    # Create target column and filter outliers\n",
    "    data['duration'] = data.lpep_dropoff_datetime - data.lpep_pickup_datetime\n",
    "    data['duration'] = data.duration.dt.total_seconds() / 60\n",
    "    \n",
    "    targets = data.duration.values\n",
    "    return targets\n",
    "\n",
    "\n",
    "def filter_target_outliers(data, targets, y_min=1, y_max=60):\n",
    "    \"\"\"Filter data with targets outside of range.\"\"\"\n",
    "\n",
    "    X = data[(data.duration >= y_min) & (data.duration <= y_max)]\n",
    "    y = X.duration.values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def plot_duration_distribution(model, X_train, y_train, X_valid, y_valid):\n",
    "    \"\"\"Plot true and prediction distribution.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "    sns.histplot(model.predict(X_train), ax=ax[0], label='pred', color='C0', stat='density', kde=True)\n",
    "    sns.histplot(y_train, ax=ax[0], label='true', color='C1', stat='density', kde=True)\n",
    "    ax[0].set_title(\"Train\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    sns.histplot(model.predict(X_valid), ax=ax[1], label='pred', color='C0', stat='density', kde=True)\n",
    "    sns.histplot(y_valid, ax=ax[1], label='true', color='C1', stat='density', kde=True)\n",
    "    ax[1].set_title(\"Valid\")\n",
    "    ax[1].legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def preprocess_datasets(train_data_path, valid_data_path):\n",
    "    \"\"\"Preprocess datasets for model training and validation. Save pipeline.\"\"\"\n",
    " \n",
    "    X_train = pd.read_parquet(train_data_path)\n",
    "    X_valid = pd.read_parquet(valid_data_path)\n",
    "    \n",
    "    # Compute labels\n",
    "    y_train = compute_targets(X_train)\n",
    "    y_valid = compute_targets(X_valid)\n",
    "\n",
    "    # Filter train and valid (!) data. (i.e. only validate on t=(1, 60) range.)\n",
    "    X_train, y_train = filter_target_outliers(X_train, y_train)\n",
    "    X_valid, y_valid = filter_target_outliers(X_valid, y_valid)\n",
    "    \n",
    "    # Feature selection and engineering\n",
    "    categorical = ['PU_DO']\n",
    "    numerical = ['trip_distance']\n",
    "\n",
    "    feature_pipe = make_pipeline(\n",
    "        PrepareFeatures(categorical, numerical),\n",
    "        DictVectorizer(),\n",
    "    )\n",
    "\n",
    "    # Fit only on train set\n",
    "    feature_pipe.fit(X_train, y_train)\n",
    "    X_train = feature_pipe.transform(X_train)\n",
    "    X_valid = feature_pipe.transform(X_valid)\n",
    "\n",
    "    # Save preprocessor\n",
    "    joblib.dump(feature_pipe, artifacts / 'preprocessor.pkl')\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a55a0d1272a360f93e747858d443ec26da69f69eac36db3e567a961ca624a861"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
