{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Updating&color=blue)\n",
    "[![Source](https://img.shields.io/static/v1.svg?label=GitHub&message=Source&color=181717&logo=GitHub)](https://github.com/particle1331/ok-transformer/blob/master/docs/nb/mlops/04-deployment)\n",
    "[![Stars](https://img.shields.io/github/stars/particle1331/ok-transformer?style=social)](https://github.com/particle1331/ok-transformer)\n",
    "\n",
    "```text\n",
    "ð—”ð˜ð˜ð—¿ð—¶ð—¯ð˜‚ð˜ð—¶ð—¼ð—»: Notes for Module 4 of the MLOps Zoomcamp (2022) by DataTalks.Club.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, we will look into deploying the ride duration model which has been our working example in the modules. Deploying means that other applications can get predictions from our model. We will look at three modes of deployment: **online** deployment, **offline** or batch deployment, and **streaming**. \n",
    "\n",
    "In online mode, our service must be up all the time. To do this, we implement a web service which takes in HTTP requests and sends out predictions. In offline or mode, we have a service running regularly, but not necessarily all the time. This can make predictions for a batch of examples that runs periodically using workflow orchestration. Finally, we look at how to implement a streaming service, i.e. a machine learning service that listens to a stream of events and reacts to it using AWS Kinesis and AWS Lambda."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying models with Flask and Docker\n",
    "\n",
    "In this section, we develop a web server using Flask for serving model predictions. The model is obtained from an S3 artifacts store and predicts on data sent to the service by the backend. We will containerize the application using Docker. This container can be deployed anywhere where Docker is supported such as Kubernetes and Elastic Beanstalk."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packaging modeling code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will collect code around our models into a [`ride_duration`](https://pypi.org/project/ride-duration-prediction/) package that will be uploaded to PyPI (hence can be installed using `pip`). Having a model package ensures smooth integration with the Flask API since we have the same code for training, testing, and inference. Ideally, we should also use the same environment for each of these phases and this can be done by using [Pipenv](https://pipenv.pypa.io/en/latest/). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, packaging makes imports just work. For consistency, we will also use this package for our batch scoring workflow. The directory structure of our project would look like the following. Notice the nice separation between model code, application code, and deployment code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "deployment/\n",
    "â”œâ”€â”€ app/\n",
    "â”‚   â””â”€â”€ main.py\n",
    "â”œâ”€â”€ ride_duration/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ predict.py\n",
    "â”‚   â”œâ”€â”€ utils.py\n",
    "â”‚   â””â”€â”€ VERSION\n",
    "â”œâ”€â”€ .env\n",
    "â”œâ”€â”€ Dockerfile\n",
    "â”œâ”€â”€ Pipfile\n",
    "â”œâ”€â”€ MANIFEST.in\n",
    "â”œâ”€â”€ Pipfile.lock\n",
    "â”œâ”€â”€ setup.py\n",
    "â”œâ”€â”€ test.py\n",
    "â”œâ”€â”€ train.py\n",
    "â””â”€â”€ pyproject.toml\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create `setup.py` and `pyproject.toml` for packaging. For the `setup` module, we only have to change the package metadata. Setting `REQUIRES_PYTHON` to `~=3.9.0` since some packages do not work with the latest version. All imports made in the module scripts are gathered in `INSTALL_REQUIRES` which contains abstract requirements minimally needed to run the package. This is [a bit different](https://packaging.python.org/en/latest/discussions/install-requires-vs-requirements/#requirements-files) from a requirements file which lists pinned versions for the purpose of achieving repeatable installations of a complete environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`setup.py`](https://github.com/particle1331/ok-transformer/blob/dee935715209f55ac833ec6c0067a43ded9629d9/docs/nb/mlops/04-deployment/setup.py#L5-L24)\n",
    "```\n",
    "```python\n",
    "# Package meta-data.\n",
    "NAME = \"ride-duration-prediction\"\n",
    "PACKAGE_NAME = \"ride_duration\"\n",
    "DESCRIPTION = \"Predicting ride duration for TLC Trip Record Data.\"\n",
    "URL = \"https://particle1331.github.io/ok-transformer/nb/mlops/04-deployment/notes.html\"\n",
    "EMAIL = \"particle1331@gmail.com\"\n",
    "AUTHOR = \"Ron Medina\"\n",
    "REQUIRES_PYTHON = \">=3.9.0,<3.10\"\n",
    "INSTALL_REQUIRES = [\n",
    "    \"scikit-learn==1.0.2\", \n",
    "    \"mlflow>=1.26.1,<1.27.0\", \n",
    "    \"pandas>=1.4.3,<1.5.0\",\n",
    "    \"joblib>=1.1.0,<1.2.0\"\n",
    "]\n",
    "LICENSE = \"MIT\"\n",
    "TROVE_CLASSIFIERS = [\n",
    "    # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers\n",
    "    \"License :: OSI Approved :: MIT License\",\n",
    "    \"Programming Language :: Python :: 3.9\",\n",
    "]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other one just specifies the build system to use:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`pyproject.toml`](https://github.com/particle1331/ok-transformer/blob/dee935715209f55ac833ec6c0067a43ded9629d9/docs/nb/mlops/04-deployment/pyproject.toml)\n",
    "```\n",
    "```python\n",
    "[build-system]\n",
    "requires = [\"setuptools>=42.0\", \"wheel\"]\n",
    "build-backend = \"setuptools.build_meta\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally we include [`MANIFEST.in`](https://github.com/particle1331/ok-transformer/blob/217134c84bb323452bf0dc3e8b6a6a04fea8f06b/docs/nb/mlops/04-deployment/MANIFEST.in) file to specify the files included in the source distribution of the package. The full list can be viewed in the `SOURCES.txt` file of the generated `egg-info` folder after package build."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`MANIFEST.in`](https://github.com/particle1331/ok-transformer/blob/dee935715209f55ac833ec6c0067a43ded9629d9/docs/nb/mlops/04-deployment/MANIFEST.in)\n",
    "```\n",
    "```\n",
    "include ride_duration/*.py\n",
    "include ride_duration/VERSION\n",
    "\n",
    "recursive-exclude * __pycache__\n",
    "recursive-exclude * *.py[co]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment with Pipenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dependency management, we will use [Pipenv](https://pipenv.pypa.io/en/latest/). Here we specify the path to the Python interpreter that the Pipenv environment will use. Recall that an easy way of installing specific versions of Python is through `conda` so we use that:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "(base) ubuntu@ip-172-31-24-183:~$ conda activate py39\n",
    "(py39) ubuntu@ip-172-31-24-183:~$ which python\n",
    "/home/ubuntu/anaconda3/envs/py39/bin/python\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deactivate the `conda` environment and navigate to the project root folder. Pointing to this path when activating the environment:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pipenv shell --python=/home/ubuntu/anaconda3/envs/py39/bin/python\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a `Pipfile` which supersedes the usual requirements file and also a `Pipfile.lock` containing hashes of downloaded packages that ensure reproducible builds. Another method is to call `pipenv install` on a folder with an existing valid `Pipfile`. To exit the virtual environment, we simply call `exit`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies.** Pipenv automatically updates the `Pipfile` when installing dependencies:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pipenv install scikit-learn==1.0.2 flask pandas mlflow boto3 \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dev dependencies.** Installing the local model package and other development dependencies:\n",
    "\n",
    "```bash\n",
    "pipenv install --dev -e .\n",
    "pipenv install --dev requests\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter.** To make this available as a kernel in Jupyter notebook or VS Code:\n",
    "\n",
    "```bash\n",
    "pipenv install --dev jupyter notebook\n",
    "pipenv run python -m ipykernel install --user --name=`basename $VIRTUAL_ENV`\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `Pipfile` should now look as follows. Note that `ride-duration-prediction` is installed in editable mode since the package code is still in development (just so imports work everywhere).\n",
    "\n",
    "```ini\n",
    "[[source]]\n",
    "url = \"https://pypi.org/simple\"\n",
    "verify_ssl = true\n",
    "name = \"pypi\"\n",
    "\n",
    "[packages]\n",
    "scikit-learn = \"==1.0.2\"\n",
    "flask = \"*\"\n",
    "pandas = \"*\"\n",
    "mlflow = \"*\"\n",
    "boto3 = \"*\"\n",
    "\n",
    "[dev-packages]\n",
    "ride-duration-prediction = {editable = true, path = \".\"}\n",
    "jupyter = \"*\"\n",
    "notebook = \"*\"\n",
    "requests = \"*\"\n",
    "\n",
    "[requires]\n",
    "python_version = \"3.9\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Secrets.** AWS credentials and other environmental variables are saved in a `.env` file in the same directory as Pipfile. These are automatically detected and loaded by Pipenv when calling `pipenv shell`. But the shell must be restarted whenever the `.env` file is modified to load the changes. Here we use [named profiles](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html) for access credentials."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# .env\n",
    "EXPERIMENT_ID=1\n",
    "RUN_ID=f4e2242a53a3410d89c061d1958ae70a\n",
    "AWS_PROFILE=mlops\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package modules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following module, define helper functions for model training and inference. This includes the usual `load_training_dataframe` function which creates the target features (ride duration in minutes) and filters it to some range, i.e. `[1, 60]`. This function is used for creating training and validation datasets. The other function `prepare_features` is used for feature engineering. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`utils.py`](https://github.com/particle1331/ok-transformer/blob/dee935715209f55ac833ec6c0067a43ded9629d9/docs/nb/mlops/04-deployment/ride_duration/utils.py#L16-L37)\n",
    "```\n",
    "```python\n",
    "def load_training_dataframe(file_path, y_min=1, y_max=60):\n",
    "    \"\"\"Load data from disk and preprocess for training.\"\"\"\n",
    "    \n",
    "    # Load data from disk\n",
    "    data = pd.read_parquet(file_path)\n",
    "\n",
    "    # Create target column and filter outliers\n",
    "    data['duration'] = data.lpep_dropoff_datetime - data.lpep_pickup_datetime\n",
    "    data['duration'] = data.duration.dt.total_seconds() / 60\n",
    "    data = data[(data.duration >= y_min) & (data.duration <= y_max)]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_features(input_data: Union[list[dict], pd.DataFrame]):\n",
    "    \"\"\"Prepare features for dict vectorizer.\"\"\"\n",
    "\n",
    "    X = pd.DataFrame(input_data)\n",
    "    X['PU_DO'] = X['PULocationID'].astype(str) + '_' + X['DOLocationID'].astype(str)\n",
    "    X = X[['PU_DO', 'trip_distance']].to_dict(orient='records')\n",
    "    \n",
    "    return X\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at the `predict` module of our package. This contains two functions: one for loading the model and another for making predictions with the model. The `load_model` function loads a model directly from the S3 artifacts store:\n",
    "\n",
    "```{margin}\n",
    "[`predict.py`](https://github.com/particle1331/ok-transformer/blob/dee935715209f55ac833ec6c0067a43ded9629d9/docs/nb/mlops/04-deployment/ride_duration/predict.py#L10-L16)\n",
    "```\n",
    "```python\n",
    "def load_model(experiment_id, run_id):\n",
    "    \"\"\"Get model from our S3 artifacts store.\"\"\"\n",
    "\n",
    "    source = f\"s3://mlflow-models-ron/{experiment_id}/{run_id}/artifacts/model\"\n",
    "    model = mlflow.pyfunc.load_model(source)\n",
    "\n",
    "    return model\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having to load the preprocessor separately from the artifacts store, we train models that are pipelines of the following form (see **Appendix** below):\n",
    "\n",
    "```python\n",
    "pipeline = make_pipeline(\n",
    "    DictVectorizer(), \n",
    "    RandomForestRegressor(**params, n_jobs=-1)\n",
    ")\n",
    "```\n",
    "\n",
    "Our models expect as input `prepare_features(data)` where `data` can be a `DataFrame` with rows containing ride or a list of ride features dictionaries (e.g obtained as a JSON payload). So we define the following function to help with inference:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`predict.py`](https://github.com/particle1331/ok-transformer/blob/dee935715209f55ac833ec6c0067a43ded9629d9/docs/nb/mlops/04-deployment/ride_duration/predict.py#L19-L25)\n",
    "```\n",
    "```python\n",
    "def make_prediction(model, input_data: Union[list[dict], pd.DataFrame]):\n",
    "    \"\"\"Make prediction from features dict or DataFrame.\"\"\"\n",
    "    \n",
    "    X = prepare_features(input_data)\n",
    "    preds = model.predict(X)\n",
    "\n",
    "    return preds\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing out the `load_model()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlflow.pyfunc.loaded_model:\n",
       "  artifact_path: model\n",
       "  flavor: mlflow.sklearn\n",
       "  run_id: f4e2242a53a3410d89c061d1958ae70a"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from ride_duration.predict import load_model\n",
    "\n",
    "model = load_model(os.getenv(\"EXPERIMENT_ID\"), os.getenv(\"RUN_ID\"))\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/s3-artifacts-ss.png\n",
    "---\n",
    "---\n",
    "Artifacts store for model runs of experiment 1.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative, we can also load the latest **production version** directly from the model registry (no need to specify a run and experiment ID) using the tracking server. One issue is that starting of the Flask server can fail whenever the request the tracking server is down.\n",
    "\n",
    "```python\n",
    "TRACKING_URI = f\"http://{TRACKING_SERVER_HOST}:5000\"\n",
    "\n",
    "# Fetch production model from client\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "client = MlflowClient(tracking_uri=TRACKING_URI)\n",
    "prod_model = client.get_latest_versions(name='NYCRideDurationModel', stages=['Production'])[0]\n",
    "\n",
    "# Load model from S3 artifacts store\n",
    "run_id = prod_model.run_id\n",
    "source = prod_model.source\n",
    "model = mlflow.pyfunc.load_model(source)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and upload\n",
    "\n",
    "Once we are satisfied with the model package, we can now upload it to PyPI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the package and uploading to PyPI:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pipenv install --dev build\n",
    "pipenv install --dev twine\n",
    "pipenv run python -m build\n",
    "export USERNAME=particle1331\n",
    "export PASSWORD=************\n",
    "pipenv run python -m twine upload -u $USERNAME -p $PASSWORD dist/*\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploaded package can be viewed [here](https://pypi.org/project/ride-duration-prediction/0.1.0/). Properly installing the package as a dependency:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "pipenv uninstall --dev -e .\n",
    "pipenv install ride-duration-prediction==0.3.0\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final `Pipfile` should look like:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`Pipfile`](https://github.com/particle1331/ok-transformer/blob/23aaa1ff9fee905e5e3e6a44cd141a41dfa5e5a3/docs/nb/mlops/04-deployment/Pipfile#L1-L26)\n",
    "```\n",
    "```ini\n",
    "[[source]]\n",
    "url = \"https://pypi.org/simple\"\n",
    "verify_ssl = true\n",
    "name = \"pypi\"\n",
    "\n",
    "[packages]\n",
    "scikit-learn = \"==1.0.2\"\n",
    "flask = \"*\"\n",
    "pandas = \"*\"\n",
    "mlflow = \"*\"\n",
    "boto3 = \"*\"\n",
    "ride-duration-prediction = \"==0.3.0\"\n",
    "prefect = \"==2.0b5\"\n",
    "python-dotenv = \"*\"\n",
    "pyarrow = \"*\"\n",
    "\n",
    "[dev-packages]\n",
    "jupyter = \"*\"\n",
    "notebook = \"*\"\n",
    "requests = \"*\"\n",
    "twine = \"*\"\n",
    "build = \"*\"\n",
    "\n",
    "[requires]\n",
    "python_version = \"3.9\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/pypi.png\n",
    "---\n",
    "width: 40em\n",
    "---\n",
    "Our model package in the Python package index. ðŸ\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving predictions using Flask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model loads when the Flask server starts. The server exposes a single endpoint defined by `predict_endpoint` which expects a singleton JSON payload of ride features from the backend. For observability, each prediction is returned along with the `run_id` of the model.\n",
    "\n",
    "```{margin}\n",
    "[`app/main.py`](https://github.com/particle1331/ok-transformer/blob/dee935715209f55ac833ec6c0067a43ded9629d9/docs/nb/mlops/04-deployment/app/main.py)\n",
    "```\n",
    "```python\n",
    "import os\n",
    "from ride_duration.predict import load_model, make_prediction\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "\n",
    "# Load model with run ID and experiment ID defined in the env.\n",
    "RUN_ID = os.getenv(\"RUN_ID\")\n",
    "EXPERIMENT_ID = os.getenv(\"EXPERIMENT_ID\")\n",
    "model = load_model(run_id=RUN_ID, experiment_id=EXPERIMENT_ID)\n",
    "\n",
    "app = Flask('duration-prediction')\n",
    "\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict_endpoint():\n",
    "    \"\"\"Predict duration of a single ride using NYCRideDurationModel.\"\"\"\n",
    "    \n",
    "    ride = request.get_json()\n",
    "    preds = make_prediction(model, [ride])\n",
    "\n",
    "    return jsonify({\n",
    "        'duration': float(preds[0]),\n",
    "        'model_version': RUN_ID,\n",
    "    })\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, host='0.0.0.0', port=9696)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing the prediction endpoint, we have the following script. This can be used without modification to test remote hosts using port forwarding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`test.py`](https://github.com/particle1331/ok-transformer/blob/dee935715209f55ac833ec6c0067a43ded9629d9/docs/nb/mlops/04-deployment/test.py)\n",
    "```\n",
    "```python\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "ride = {\n",
    "    \"PULocationID\": 130,\n",
    "    \"DOLocationID\": 205,\n",
    "    \"trip_distance\": 3.66,\n",
    "}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    host = \"http://0.0.0.0:9696\"\n",
    "    url = f\"{host}/predict\"\n",
    "    response = requests.post(url, json=ride)\n",
    "    result = response.json()\n",
    "    \n",
    "    print(result)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containerizing the application with Docker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our `Dockerfile`, we start by installing Pipenv. Then, we copy all files relevant for running the application. So these are the requirements files, the model package files, and the files for the Flask app. Next, we install everything using Pipenv, expose the `9696` endpoint, and configure the entrypoint which serves the main app on `0.0.0.0:9696`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`Dockerfile`](https://github.com/particle1331/ok-transformer/blob/dee935715209f55ac833ec6c0067a43ded9629d9/docs/nb/mlops/04-deployment/Dockerfile)\n",
    "```\n",
    "```Dockerfile\n",
    "FROM python:3.9.13-slim\n",
    "\n",
    "RUN pip install -U pip\n",
    "RUN pip install pipenv\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY [ \"Pipfile\", \"Pipfile.lock\",  \"./\"]\n",
    "COPY [ \"app\",  \"./app\"]\n",
    "\n",
    "RUN pipenv install --system --deploy\n",
    "\n",
    "EXPOSE 9696\n",
    "\n",
    "# https://stackoverflow.com/a/71092624/1091950\n",
    "ENTRYPOINT [ \"gunicorn\", \"--bind=0.0.0.0:9696\", \"--timeout=600\", \"app.main:app\" ]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the image:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker build -t ride-duration-prediction-service:v1 .\n",
    "```\n",
    "```bash\n",
    "[+] Building 177.1s (12/12) FINISHED\n",
    " => [internal] load build definition from Docke  0.1s\n",
    " => => transferring dockerfile: 376B             0.0s\n",
    " => [internal] load .dockerignore                0.1s\n",
    " => => transferring context: 2B                  0.0s\n",
    " => [internal] load metadata for docker.io/libr  4.9s\n",
    " => [1/7] FROM docker.io/library/python:3.9.13  22.0s\n",
    " => => resolve docker.io/library/python:3.9.13-  0.0s\n",
    " => => sha256:278d211701f68 858.90kB / 858.90kB  3.1s\n",
    " => => sha256:94b65918aca5f4b 11.59MB / 11.59MB  7.6s\n",
    " => => sha256:c01a2db78654c1923 1.86kB / 1.86kB  0.0s\n",
    " => => sha256:ef78109875ac0ac47 1.37kB / 1.37kB  0.0s\n",
    " => => sha256:ddaaa6b80b0541f48 7.50kB / 7.50kB  0.0s\n",
    " => => sha256:3b157c852f2736 30.07MB / 30.07MB  15.8s\n",
    " => => sha256:0f660d3f04731206fb1e8 234B / 234B  3.9s0\n",
    " => => sha256:2b5c279a2d3443d3a 2.95MB / 2.95MB  8.6s0\n",
    " => => extracting sha256:3b157c852f2736e12f0904  3.4s0\n",
    " => => extracting sha256:278d211701f6882403e47f  0.3s0\n",
    " => => extracting sha256:94b65918aca5f4bc0654e6  0.9s0\n",
    " => => extracting sha256:0f660d3f04731206fb1e8b  0.0s\n",
    " => => extracting sha256:2b5c279a2d3443d3ab6534  0.5s\n",
    " => [internal] load build context                0.1s\n",
    " => => transferring context: 96.05kB             0.0s\n",
    " => [2/7] RUN pip install -U pip                 5.7s\n",
    " => [3/7] RUN pip install pipenv                14.1s\n",
    " => [4/7] WORKDIR /app                           0.1s\n",
    " => [5/7] COPY [ Pipfile, Pipfile.lock,  ./]     0.0s\n",
    " => [6/7] COPY [ app,  ./app]                    0.0s\n",
    " => [7/7] RUN pipenv install --system --deplo  106.1s\n",
    " => exporting to image                          23.7s\n",
    " => => exporting layers                         23.4s\n",
    " => => writing image sha256:f92279e7eb9164ba32e  0.0s\n",
    " => => naming to docker.io/library/ride-duratio  0.0s\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the container with variables in `.env` loaded as environmental variables:\n",
    "\n",
    "```bash\n",
    "docker run --env-file .env -it --rm -p 9696:9696 ride-duration-prediction-service:v1\n",
    "```\n",
    "```text\n",
    "[2022-06-25 23:51:36 +0000] [1] [INFO] Starting gunicorn 20.1.0\n",
    "[2022-06-25 23:51:36 +0000] [1] [INFO] Listening at: http://0.0.0.0:9696 (1)\n",
    "[2022-06-25 23:51:36 +0000] [1] [INFO] Using worker: sync\n",
    "[2022-06-25 23:51:36 +0000] [7] [INFO] Booting worker with pid: 7\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'duration': 18.210770674183355, 'model_version': 'f4e2242a53a3410d89c061d1958ae70a'}\n"
     ]
    }
   ],
   "source": [
    "!python test.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that after the initial loading time the next predictions are returned instantaneously. This confirms that the model is loaded only once when the server starts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming: Deploying models with Kinesis and Lambda\n",
    "\n",
    "A streaming service consists of **producers** and **consumers**. Producers push events to the event stream which are consumed by consuming services that react to this stream. Recall that a web service exhibits a 1-1 relationship so that there is explicit connection between user and service. On the other hand, the relationship between producing and consuming services can be 1-many or many-many. There is only implicit connection since we don't know which consumers will react or how many. Streaming services can be scaled to many services or models.\n",
    "\n",
    "For example, when a user uses our ride hailing app, the backend can send an event to the stream containing all information about this ride. Then services will react on this event, e.g. one consuming service predicts tip and sends a push notification to user asking for the tip. And consuming services which makes better ride duration prediction but takes more time to make a prediction can update the prediction that was initially given to the user by the online web service. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/lambda.png\n",
    "---\n",
    "---\n",
    "**Streaming service that we develop in this section.** A production model is pulled from S3 and served as a function in AWS Lambda. This function reads on the ride events stream and writes on the ride predictions stream. Some elements around this system that can be present in actual production systems are in dashed outlines. For example, a Python microservice is shown that reads and writes on the same streams as our Lambda function.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an IAM Role"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a serverless function for serving our model, we will use [AWS Lambda](https://aws.amazon.com/lambda/). The advantage of this is that we do not have to worry about owning a server that runs our function, we just know that the function is being executed somewhere in AWS. For the sake of demonstration, we will pretend that we are serving better model predictions, although we are actually deploying the same Random Forest model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps**\n",
    "\n",
    "1. Open the roles page in the IAM console and choose **Create role**. Choose Lambda as trusted entity type.\n",
    "    ```{figure} ../../../img/create-role.png\n",
    "    ---\n",
    "    ---\n",
    "    ```\n",
    "\n",
    "2. Add `AWSLambdaKinesisExecutionRole` to permissions. This role can read from Kinesis streams and write logs. Resource `\"*\"` means that the role can do this to *any* Kinesis stream or log group. So this takes care of reading from input streams for our functions.\n",
    "    ```{figure} ../../../img/create-role-2.png\n",
    "    ---\n",
    "    ---\n",
    "    ```\n",
    "\n",
    "3. Set name to `lambda-kinesis-role` and proceed to create role.\n",
    "    ```{figure} ../../../img/create-role-3.png\n",
    "    ---\n",
    "    ---\n",
    "    ```\n",
    "\n",
    "4. Next we will give read permissions to S3. This is because our Lambda function will get its machine learning model from the MLflow artifacts store in S3. Select the `lambda-kinesis-role`, then select **Add permissions** > **Attach policies** > **Create policy**. Switch to the editor's JSON tab and paste:\n",
    "    ```json\n",
    "    {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"VisualEditor0\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"s3:Get*\",\n",
    "                    \"s3:List*\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    \"arn:aws:s3:::mlflow-models-ron\",\n",
    "                    \"arn:aws:s3:::mlflow-models-ron/*\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    ```\n",
    "    Here `mlflow-models-ron` is the name of the artifacts store in S3. From this JSON, we can read that this policy allows all `Get` and all `List` actions on our `mlflow-models-ron` S3 bucket and its subdirectories. Skip tags, set name to be `read_premission_mlflow-models-ron`, put in the appropriate description, then select **Create policy**. Make sure to attach this policy to `lambda-kinesis-role`.\n",
    "    <br> <br>\n",
    "\n",
    "5. Finally, our Lambda functions have to write to output data streams. Do the same as above and create and attach a policy to `lambda-kinesis-role`. In this case, paste the following JSON, and name the policy `lambda_kinesis_write_to_ride_predictions`.\n",
    "    ```json\n",
    "    {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"VisualEditor0\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"kinesis:PutRecord\",\n",
    "                    \"kinesis:PutRecords\"\n",
    "                ],\n",
    "                \"Resource\": \"arn:aws:kinesis:us-east-1:241297376613:stream/ride_predictions\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    This defines permissions for writing on the `ride_predictions` data stream (which does not exist yet). Here `us-east-1` is the region and `241297376613` is the IAM Account ID. Note that all streams and functions must live on the same region for them to work together. The allowed actions are `PutRecord` for an API call that puts a single record on the stream, and `PutRecords` for an API call that puts a batch of records on the stream."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/lambda-kinesis-policies.png\n",
    "---\n",
    "---\n",
    "The `lambda-kinesis-role` for our Lambda function with policies for reading from all Kinesis data streams, writing on `ride_predictions` Kinesis data stream, and getting models from the `mlflow-models-ron` bucket in S3. \n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Lambda function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to Lambda and create `ride-duration-prediction-test` for testing. Later we will create the actual function. In the permissions, choose the role that we have just created.\n",
    "\n",
    "```{figure} ../../../img/create-function.png\n",
    "---\n",
    "---\n",
    "```\n",
    "\n",
    "```{figure} ../../../img/create-function-2.png\n",
    "---\n",
    "---\n",
    "The `ride-duration-prediction-test` function.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify this to look more like our machine learning function. Note that unlike for a web service, there is no 1-1 relationship between inputs and outputs. We have to include a `ride_id` to tie an input event to its corresponding event in the output stream. In the code section write and deploy:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# lambda_function.py\n",
    "import json\n",
    "\n",
    "def prepare_features(ride):\n",
    "    features = {}\n",
    "    features['PU_DO'] = f\"{ride['PULocationID']}_{ride['DOLocationID']}\"\n",
    "    features['trip_distance'] = ride['trip_distance']\n",
    "    return features\n",
    "\n",
    "def predict(features):\n",
    "    return 10.0\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    ride = event['ride']\n",
    "    ride_id = event['ride_id']\n",
    "\n",
    "    features = prepare_features(ride)\n",
    "    prediction = predict(features)\n",
    "\n",
    "    return {\n",
    "        'ride_duration': prediction,\n",
    "        'ride_id': ride_id\n",
    "    }\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the **test event** to:\n",
    "\n",
    "```{margin}\n",
    "**Toy test event**\n",
    "```\n",
    "```JSON\n",
    "{\n",
    "    \"ride\": {\n",
    "        \"PULocationID\": 130,\n",
    "        \"DOLocationID\": 205,\n",
    "        \"trip_distance\": 3.66\n",
    "    }, \n",
    "    \"ride_id\": 123\n",
    "}\n",
    "```\n",
    "\n",
    "Running the test. The model predicts a constant `10` minutes for each ride:\n",
    "\n",
    "```{figure} ../../../img/lambda-2.png\n",
    "---\n",
    "---\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading input a Kinesis data stream"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we attach a Kinesis data stream to our function. Go to Kinesis to create a data stream. We will call it `ride_events` and set its capacity mode to **Provisioned** with 1 shard. AWS provides the write and read capacity for the chosen number of shards. Note that we have to pay per hour for each shard.\n",
    "\n",
    "```{figure} ../../../img/kinesis-stream-1.png\n",
    "---\n",
    "width: 30em\n",
    "---\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to Lambda and add `ride_events` as trigger to the function:\n",
    "\n",
    "```{figure} ../../../img/kinesis-stream-2.png\n",
    "---\n",
    "width: 30em\n",
    "---\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the trigger is enabled, we execute the following the command line which puts a test event to the stream. Our lambda should read from the stream and we should see this in logs. We comment out our code, and only print out what a Kinesis event looks like:\n",
    "\n",
    "```python\n",
    "# lambda_function.py\n",
    "import json\n",
    "\n",
    "def prepare_features(ride):\n",
    "    features = {}\n",
    "    features['PU_DO'] = f\"{ride['PULocationID']}_{ride['DOLocationID']}\"\n",
    "    features['trip_distance'] = ride['trip_distance']\n",
    "    return features\n",
    "\n",
    "def predict(features):\n",
    "    return 10.0\n",
    "\n",
    "def lambda_handler(event, context):  \n",
    "    event_json = json.dumps(event)\n",
    "    print(event_json)\n",
    "\n",
    "    # ride = event['ride']\n",
    "    # ride_id = event['ride_id']\n",
    "\n",
    "    # features = prepare_features(ride)\n",
    "    # prediction = predict(features)\n",
    "\n",
    "    # return {\n",
    "    #     'ride_duration': prediction,\n",
    "    #     'ride_id': ride_id\n",
    "    # }\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the above test event into the input stream:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "**Put record**\n",
    "<br>\n",
    "v. `aws-cli/1.22.34`\n",
    "```\n",
    "```bash\n",
    "aws kinesis put-record \\\n",
    "    --stream-name ride_events \\\n",
    "    --partition-key 1 \\\n",
    "    --data '{\n",
    "        \"ride\": {\n",
    "            \"PULocationID\": 130,\n",
    "            \"DOLocationID\": 205,\n",
    "            \"trip_distance\": 3.66\n",
    "        },\n",
    "        \"ride_id\": 123\n",
    "    }'\n",
    "```\n",
    "```{margin}\n",
    "`[out]`\n",
    "```\n",
    "```bash\n",
    "{\n",
    "    \"ShardId\": \"shardId-000000000000\",\n",
    "    \"SequenceNumber\": \"49630706038424016596026506533783680704960320015674900482\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `SequenceNumber` of this event. This event can be found on the logs by looking at the \"Monitor\" tab of the lambda function and clicking on \"View logs in CloudWatch\". We update the test event in the lambda function with this record. In particular, this means we don't have to push events to the input stream when testing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "**Actual test event**\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"Records\": [\n",
    "        {\n",
    "            \"kinesis\": {\n",
    "                \"kinesisSchemaVersion\": \"1.0\",\n",
    "                \"partitionKey\": \"1\",\n",
    "                \"sequenceNumber\": \"49630706038424016596026506533782471779140474214180454402\",\n",
    "                \"data\": \"eyAgICAgICAgICAicmlkZSI6IHsgICAgICAgICAgICAgICJQVUxvY2F0aW9uSUQiOiAxMzAsICAgICAgICAgICAgICAiRE9Mb2NhdGlvbklEIjogMjA1LCAgICAgICAgICAgICAgInRyaXBfZGlzdGFuY2UiOiAzLjY2ICAgICAgICAgIH0sICAgICAgICAgICJyaWRlX2lkIjogMTIzICAgICAgfQ==\",\n",
    "                \"approximateArrivalTimestamp\": 1655944485.718\n",
    "            },\n",
    "            \"eventSource\": \"aws:kinesis\",\n",
    "            \"eventVersion\": \"1.0\",\n",
    "            \"eventID\": \"shardId-000000000000:49630706038424016596026506533782471779140474214180454402\",\n",
    "            \"eventName\": \"aws:kinesis:record\",\n",
    "            \"invokeIdentityArn\": \"arn:aws:iam::241297376613:role/lambda-kinesis-role\",\n",
    "            \"awsRegion\": \"us-east-1\",\n",
    "            \"eventSourceARN\": \"arn:aws:kinesis:us-east-1:241297376613:stream/ride_events\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/kinesis-stream-5.png\n",
    "---\n",
    "---\n",
    "Record with sequence number `496...402` printed in the logs.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that Kinesis encodes event data in `base64`, i.e. as `\"data\": \"eyAgI...\"`. So we have to decode this to be able to use it. Since the batch size is set to 100, we need to iterate over records to access each of them. Our function is modified as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# lambda_function.py\n",
    "...\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    for record in event['Records']:\n",
    "        encoded_data = record['kinesis']['data']\n",
    "        decoded_data = base64.b64decode(encoded_data).decode('utf-8')\n",
    "        ride_event = json.loads(decoded_data)\n",
    "\n",
    "        ride = ride_event['ride']\n",
    "        ride_id = ride_event['ride_id']\n",
    "    \n",
    "        features = prepare_features(ride)\n",
    "        prediction = predict(features)\n",
    "    \n",
    "        return {\n",
    "            'ride_duration': prediction,\n",
    "            'ride_id': ride_id\n",
    "        }\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing on the Kinesis test event. Observe that the data has been properly decoded:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/kinesis-stream-6.png\n",
    "---\n",
    "---\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing predictions to a Kinesis data stream"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we created an input data stream in Kinesis. Repeat the same process in Kinesis to create an output data stream called `ride_predictions`. Similarly set its capacity mode to **Provisioned** with 1 shard. Note that we already created write permission to this stream even before it was created. But you have to get the name correct.\n",
    "\n",
    "To write onto an `ride_predictions`, we modify our lambda function as follows. Basically, we have a client version of the CLI command for putting records onto a data stream. So we have to connect to the Kinesis client using `boto3` and specify `PREDICTIONS_STREAM_NAME` which defaults to `ride_predictions` which has just been created. Paste the following in the code editor:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# lambda_function.py\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "kinesis_client = boto3.client('kinesis')\n",
    "PREDICTIONS_STREAM_NAME = os.getenv('PREDICTIONS_STREAM_NAME', 'ride_predictions')\n",
    "\n",
    "\n",
    "def prepare_features(ride):\n",
    "    features = {}\n",
    "    features['PU_DO'] = f\"{ride['PULocationID']}_{ride['DOLocationID']}\"\n",
    "    features['trip_distance'] = ride['trip_distance']\n",
    "    return features\n",
    "    \n",
    "    \n",
    "def predict(features):\n",
    "    return 10.0\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    prediction_events = []\n",
    "\n",
    "    for record in event['Records']:\n",
    "        encoded_data = record['kinesis']['data']\n",
    "        decoded_data = base64.b64decode(encoded_data).decode('utf-8')\n",
    "        \n",
    "        ride_event = json.loads(decoded_data)\n",
    "        ride = ride_event['ride']\n",
    "        ride_id = ride_event['ride_id']\n",
    "    \n",
    "        features = prepare_features(ride)\n",
    "        prediction = predict(features)\n",
    "        \n",
    "        prediction_event = {\n",
    "            'model': 'ride_duration_prediction_model',\n",
    "            'version': 123,\n",
    "            'prediction': {\n",
    "                'ride_duration': prediction,\n",
    "                'ride_id': ride_id\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/kinesis.html#Kinesis.Client.put_record\n",
    "        kinesis_client.put_record(\n",
    "            StreamName=PREDICTIONS_STREAM_NAME,\n",
    "            Data=json.dumps(prediction_event),\n",
    "            PartitionKey=str(ride_id)\n",
    "        )\n",
    "        \n",
    "        prediction_events.append(prediction_event)\n",
    "\n",
    "    return {\n",
    "        'predictions': prediction_events\n",
    "    } \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in applications multiple consumers can push data to this output stream, so we include `model` and `version` in the output for the prediction event to be traceable to this model. Also `put_records` which can support up to 500 records is cheaper, since you pay for each API call. But to make the code simpler, we use `put_record`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing this in the Lambda UI. If you fail to create the correct write permission you'll get the following error during testing:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\"errorMessage\": \"An error occurred (AccessDeniedException) when calling the PutRecord operation: User: arn:aws:sts::241297376613:assumed-role/lambda-kinesis-role/ride-duration-prediction-test is not authorized to perform: kinesis:PutRecord on resource: arn:aws:kinesis:us-east-1:241297376613:stream/ride_predictions because no identity-based policy allows the kinesis:PutRecord action\"...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/kinesis-out-stream-1.png\n",
    "---\n",
    "---\n",
    "Test passed.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading from output stream:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "**Get records**\n",
    "<br>\n",
    "v. `aws-cli/1.22.34`\n",
    "```\n",
    "```bash\n",
    "KINESIS_STREAM_OUTPUT='ride_predictions'\n",
    "SHARD='shardId-000000000000'\n",
    "\n",
    "SHARD_ITERATOR=$(aws kinesis \\\n",
    "    get-shard-iterator \\\n",
    "        --shard-id ${SHARD} \\\n",
    "        --shard-iterator-type TRIM_HORIZON \\\n",
    "        --stream-name ${KINESIS_STREAM_OUTPUT} \\\n",
    "        --query 'ShardIterator' \\\n",
    ")\n",
    "\n",
    "RESULT=$(aws kinesis get-records --shard-iterator $SHARD_ITERATOR)\n",
    "\n",
    "echo ${RESULT} | jq -r '.Records[-1].Data' | base64 --decode | jq\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "`[out]`\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "{\n",
    "  \"model\": \"ride_duration_prediction_model\",\n",
    "  \"version\": 123,\n",
    "  \"prediction\": {\n",
    "    \"ride_duration\": 10,\n",
    "    \"ride_id\": 123\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we only have one shard. If we have multiple shards, we have to know which shard to read to. Shard iterator gives us an ID of an iterator, and that gives us a way to retrieve records from a stream. The command above is a bit complex, since we are reading without Lambda which hides all of these details under the hood. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving the model as a container with ECR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while we are able to write on the output stream, we are only able to write with fixed predictions of `10.0` minutes on a model with version `123`. Now we finally get to the most important part of adding our model. So, again, assume this model is a larger more accurate model that reads on the stream and updates the prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further improve our `lambda_function.py` by including reading environment variables and  downloading the model from S3. The `predict` function now uses the model instead of a constant for returning predictions. Here we convert the output to float instead of a singleton numpy array since the output must be in JSON format. For testing, a `TEST_RUN` flag is defined so that the function can be called without writing on the `ride_predictions` stream."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`lambda_function.py`](https://github.com/particle1331/ok-transformer/blob/7d841f80865c9773a21b2c89c4ffb30fa1c94c2b/docs/nb/mlops/04-deployment/streaming/lambda_function.py)\n",
    "```\n",
    "```python\n",
    "# lambda_function.py\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "import os\n",
    "import mlflow\n",
    "\n",
    "from ride_duration.predict import load_model\n",
    "from ride_duration.utils import prepare_features\n",
    "\n",
    "\n",
    "# Load environmental variables\n",
    "PREDICTIONS_STREAM_NAME = os.getenv('PREDICTIONS_STREAM_NAME', 'ride_predictions')\n",
    "TEST_RUN = os.getenv('TEST_RUN', 'False')\n",
    "RUN_ID = os.getenv('RUN_ID')\n",
    "EXPERIMENT_ID = os.getenv('EXPERIMENT_ID')\n",
    "\n",
    "# Load the model from S3\n",
    "model = load_model(experiment_id=EXPERIMENT_ID, run_id=RUN_ID)\n",
    "\n",
    "\n",
    "def predict(features):\n",
    "    prediction = model.predict(features)\n",
    "    return float(prediction[0])\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    prediction_events = []\n",
    "\n",
    "    for record in event['Records']:\n",
    "        encoded_data = record['kinesis']['data']\n",
    "        decoded_data = base64.b64decode(encoded_data).decode('utf-8')\n",
    "        \n",
    "        ride_event = json.loads(decoded_data)\n",
    "        ride_data = ride_event['ride']\n",
    "        ride_id = ride_event['ride_id']\n",
    "    \n",
    "        features = prepare_features([ride_data])\n",
    "        prediction = predict(features)\n",
    "        \n",
    "        prediction_event = {\n",
    "            'model': 'ride_duration_prediction_model',\n",
    "            'version': RUN_ID,\n",
    "            'prediction': {\n",
    "                'ride_duration': prediction,\n",
    "                'ride_id': ride_id\n",
    "            }\n",
    "        }\n",
    "\n",
    "        if TEST_RUN == 'False':\n",
    "            kinesis_client = boto3.client('kinesis')\n",
    "\n",
    "            # This is just the Python client version of the `aws kinesis put-record` CLI command.\n",
    "            # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/kinesis.html#Kinesis.Client.put_record\n",
    "            kinesis_client.put_record(\n",
    "                StreamName=PREDICTIONS_STREAM_NAME,\n",
    "                Data=json.dumps(prediction_event),\n",
    "                PartitionKey=str(ride_id)\n",
    "            )\n",
    "        \n",
    "        prediction_events.append(prediction_event)\n",
    "\n",
    "    return {\n",
    "        'predictions': prediction_events\n",
    "    }\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dockerfile\n",
    "\n",
    "AWS Lambda provides a [base image](https://gallery.ecr.aws/lambda/python) which contain all the required components for running the container on that platform. Note that there is no need to set a working directory and we can work on the root directory.\n",
    "\n",
    "\n",
    "```{margin}\n",
    "[`Dockerfile`](https://github.com/particle1331/ok-transformer/blob/7d841f80865c9773a21b2c89c4ffb30fa1c94c2b/docs/nb/mlops/04-deployment/streaming/Dockerfile)\n",
    "```\n",
    "```Dockerfile\n",
    "FROM public.ecr.aws/lambda/python:3.9\n",
    "\n",
    "RUN pip install -U pip\n",
    "RUN pip install pipenv\n",
    "\n",
    "COPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]\n",
    "\n",
    "RUN pipenv install --system --deploy\n",
    "\n",
    "COPY [ \"lambda_function.py\", \"./\" ]\n",
    "\n",
    "CMD [ \"lambda_function.lambda_handler\" ]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the image:\n",
    "\n",
    "```bash\n",
    "docker build -t stream-model-duration:v1 .\n",
    "```\n",
    "\n",
    "Running the image:\n",
    "```bash\n",
    "docker run -it --rm -p 8080:8080 --env-file .env stream-model-duration:v1\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/ecr-img.png\n",
    "---\n",
    "width: 40em\n",
    "---\n",
    "AWS Lambda base image for Python.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environmental variables\n",
    "\n",
    "```bash\n",
    "# .env\n",
    "TEST_RUN=True\n",
    "PREDICTIONS_STREAM_NAME=ride_predictions\n",
    "EXPERIMENT_ID=1\n",
    "RUN_ID=f4e2242a53a3410d89c061d1958ae70a\n",
    "AWS_PROFILE=mlops\n",
    "AWS_DEFAULT_REGION=us-east-1\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Lambda is a cloud service for calling functions. So to check correctness, we simply call `lambda_handler(event)` where the test event is assigned to `event` and examine the output. This is implemented in [`test.py`](https://github.com/particle1331/ok-transformer/blob/e0312f57ce4693f6ffe58bd2dbab4ac95ead08f5/docs/nb/mlops/04-deployment/streaming/test.py) which is executed below. Note that the version is now the MLflow `run_id` and predicted ride duration is different from `10.0`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ export $(cat .env | xargs)\n",
    "$ python test.py\n",
    "\n",
    "{\n",
    "    'predictions': [\n",
    "        {\n",
    "            'model': 'ride_duration_prediction_model', \n",
    "            'version': 'f4e2242a53a3410d89c061d1958ae70a', \n",
    "            'prediction': {\n",
    "                'ride_duration': 18.21077067418335, \n",
    "                'ride_id': 123\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the function definition looks to be correct, we proceed to testing whether the Docker container is able to run the function. This is implemented in [`test_docker.py`](https://github.com/particle1331/ok-transformer/blob/e0312f57ce4693f6ffe58bd2dbab4ac95ead08f5/docs/nb/mlops/04-deployment/streaming/test_docker.py) that executes a POST request on the following URL:\n",
    "\n",
    "```python\n",
    "url = 'http://localhost:8080/2015-03-31/functions/function/invocations'\n",
    "response = requests.post(url, json=event)\n",
    "print(response.json())\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `functions/function/invocations` endpoint (v. `2015-03-31`) is exposed by the base image on `8080` which we did not modify. Recall that we had to do port-forwarding when running the container. In AWS Lambda, this endpoint is automatically exposed to records on the input stream, so no further configuration is required on our part. Running the test gives the same result as above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Publishing to ECR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the container is working, we push the container into the registry. First we create a repository for our containers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "`aws-cli` <br>\n",
    "v. `1.22.34`\n",
    "```\n",
    "```bash\n",
    "aws ecr create-repository --repository-name duration-model\n",
    "```\n",
    "```json\n",
    "{\n",
    "    \"repository\": {\n",
    "        \"repositoryArn\": \"arn:aws:ecr:us-east-1:241297376613:repository/duration-model\",\n",
    "        \"registryId\": \"241297376613\",\n",
    "        \"repositoryName\": \"duration-model\",\n",
    "        \"repositoryUri\": \"241297376613.dkr.ecr.us-east-1.amazonaws.com/duration-model\",\n",
    "        \"createdAt\": 1656099951.0,\n",
    "        \"imageTagMutability\": \"MUTABLE\",\n",
    "        \"imageScanningConfiguration\": {\n",
    "            \"scanOnPush\": false\n",
    "        },\n",
    "        \"encryptionConfiguration\": {\n",
    "            \"encryptionType\": \"AES256\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/ecr-repo.png\n",
    "---\n",
    "width: 40em\n",
    "---\n",
    "The `duration-model` repository in ECR.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging in:\n",
    "```{margin}\n",
    "`aws-cli` <br>\n",
    "v. `1.22.34`\n",
    "```\n",
    "```bash\n",
    "$(aws ecr get-login --no-include-email)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pushing container to the repository:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "`docker` <br>\n",
    "v. `20.10.12` \n",
    "```\n",
    "```bash\n",
    "REMOTE_URI=241297376613.dkr.ecr.us-east-1.amazonaws.com/duration-model\n",
    "REMOTE_TAG=v1\n",
    "REMOTE_IMAGE_URI=${REMOTE_URI}:${REMOTE_TAG}\n",
    "LOCAL_IMAGE=stream-model-duration:v1\n",
    "\n",
    "docker tag ${LOCAL_IMAGE} ${REMOTE_IMAGE_URI}\n",
    "docker push ${REMOTE_IMAGE_URI}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "The push refers to repository [241297376613.dkr.ecr.us-east-1.amazonaws.com/duration-model]\n",
    "d633cfbf6042: Pushed\n",
    "1e16d3c3a5e4: Pushing  124.5MB/657MB\n",
    "593e5b91fe04: Pushed\n",
    "ed5e98d9c477: Pushed\n",
    "a5a2488932a6: Pushed\n",
    "8071867dc313: Pushed\n",
    "39978c3cb375: Pushing  223.5MB\n",
    "6ea38db36806: Pushed\n",
    "f92fb29958b6: Pushed\n",
    "f1c31f6b2603: Pushed\n",
    "fe1bfb0e592a: Pushing  115.7MB/333.9MB\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/ecr-repo-v1.png\n",
    "---\n",
    "width: 40em\n",
    "---\n",
    "The `v1` container has been pushed to the `duration-model` repository in ECR.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the lambda function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our container in ECR, we deploy this in AWS Lambda. Then, we attach the `ride_events` stream as input or trigger, and update its environmental variables. Recall that we run our container with a `.env` file during testing. We also have to update the memory allocated to the function and its allocated prediction time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```{figure} ../../../img/ecr-create-function.png\n",
    "---\n",
    "width: 40em\n",
    "---\n",
    "Creating a Lambda function based on a container. Here we need the remote image URI which we copied using the **Copy URI** button in the `v1` image in ECR.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/ecr-memory-settings.png\n",
    "---\n",
    "width: 40em\n",
    "---\n",
    "Update memory to 512 MB and timeout to 15 seconds.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/ecr-env-variables.png\n",
    "---\n",
    "width: 40em\n",
    "---\n",
    "Assigning environmental variables. Here we just transfer our `.env` files.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the test in the UI with the above **Actual test event** and execute test. Here we are testing for permission and execution time and memory. Notice that the first call takes a while, but subsequent tests are faster. This is why we initially set timeout to 15 seconds. Further tests only require 0.1 seconds."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/ecr-test-results.png\n",
    "---\n",
    "width: 40em\n",
    "---\n",
    "Execution results for testing in the UI.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further tests by modifying `ride_id` for better visibility. Here we will try to retrieve the results from the output stream. Putting an event on input stream:\n",
    "\n",
    "```bash\n",
    "aws kinesis put-record \\\n",
    "    --stream-name ride_events \\\n",
    "    --partition-key 1 \\\n",
    "    --data '{\n",
    "        \"ride\": {\n",
    "            \"PULocationID\": 130,\n",
    "            \"DOLocationID\": 205,\n",
    "            \"trip_distance\": 3.66\n",
    "        },\n",
    "        \"ride_id\": \"container_test_event\"\n",
    "    }'\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching test results from output stream:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "KINESIS_STREAM_OUTPUT='ride_predictions'\n",
    "SHARD='shardId-000000000000'\n",
    "\n",
    "SHARD_ITERATOR=$(aws kinesis \\\n",
    "    get-shard-iterator \\\n",
    "        --shard-id ${SHARD} \\\n",
    "        --shard-iterator-type TRIM_HORIZON \\\n",
    "        --stream-name ${KINESIS_STREAM_OUTPUT} \\\n",
    "        --query 'ShardIterator' \\\n",
    ")\n",
    "\n",
    "RESULT=$(aws kinesis get-records --shard-iterator $SHARD_ITERATOR)\n",
    "$ echo ${RESULT} | jq -r '.Records[-1].Data' | base64 --decode | jq\n",
    "```\n",
    "```text\n",
    "{\n",
    "  \"model\": \"ride_duration_prediction_model\",\n",
    "  \"version\": \"f4e2242a53a3410d89c061d1958ae70a\",\n",
    "  \"prediction\": {\n",
    "    \"ride_duration\": 18.210770674183355,\n",
    "    \"ride_id\": \"container_test_event\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the output is from our container. Nice! \n",
    "\n",
    "**Remark.** The obtained records can be from the test lambda function which also reads and writes on the same data streams as the container. Either delete that or wait for a few seconds and execute `get-records` again to get the expected output. This also demonstrates how we can have multiple lambda functions acting on the same stream."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying batch predictions\n",
    "\n",
    "For use cases that do not require the responsiveness of a web service, we can implement an offline service that makes batch predictions. Typically, offline services are expected to be done between fixed time periods, e.g. daily, weekly, or monthly. A critical element of this is **workflow orchestration**. In our example, we screate a workflow where we regularly pull data from a database, make predictions on that data, then write the predictions file on S3. This file can be used for analytics or for powering a chart in a dashboard."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch scoring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For batch scoring we simply load the model and use it to make batch predictions on a list of examples. After prediction, we format our output file. This includes all information relevant to modelling that can be useful for analytics and monitoring. A generated `uuid` for each row that acts as an index and the `run_id` of the model are included for observability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`score.py`](https://github.com/particle1331/ok-transformer/blob/8189857b1559d11121bb23630e223eff3b054ff5/docs/nb/mlops/04-deployment/score.py)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def save_results(df, preds, run_id, output_file):\n",
    "    \"\"\"Save output dataframe containing model predictions.\"\"\"\n",
    "\n",
    "    results_cols = [\n",
    "        'lpep_pickup_datetime', \n",
    "        'PULocationID', \n",
    "        'DOLocationID', \n",
    "        'trip_distance', \n",
    "    ]\n",
    "    \n",
    "    df_results = df[results_cols].copy()\n",
    "    df_results['actual_duration'] = df['duration']\n",
    "    df_results['predicted_duration'] = preds\n",
    "    df_results['diff'] = df_results['actual_duration'] - df_results['predicted_duration']\n",
    "    df_results['model_version']= run_id\n",
    "    df_results['ride_id'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "    \n",
    "    # Saving results\n",
    "    df_results.to_parquet(output_file, index=False)\n",
    "\n",
    "...\n",
    "\n",
    "@task\n",
    "def apply_model(input_file, output_file, experiment_id, run_id):\n",
    "    \"\"\"Load input and model. Make predictions on the input file.\"\"\"\n",
    "    \n",
    "    # Get prefect logger\n",
    "    logger = get_run_logger()\n",
    "\n",
    "    logger.info(f'Reading the data from {input_file}')\n",
    "    df = load_training_dataframe(input_file) # load from S3\n",
    "\n",
    "    logger.info(f'Loading model {experiment_id}/{run_id}')\n",
    "    model = load_model(experiment_id, run_id)\n",
    "\n",
    "    logger.info(f'Applying the model')\n",
    "    y_pred = make_prediction(model, df)\n",
    "\n",
    "    logger.info(f'Saving the result to {output_file}')\n",
    "    save_results(df, y_pred, run_id, output_file)\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that can be easily missed here is that `df_results.to_parquet(output_file)` actually writes directly to S3 whenever `output_file` has the form `\"s3://bucket/file_path\"`. In fact, this will be how we will save our output files to our S3 bucket. Let us look at an example of applying the model for February 2021:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:38:19.628 | INFO    | prefect.engine - Created flow run 'shrewd-gerbil' for flow 'test-apply-model'\n",
      "09:38:19.629 | INFO    | Flow run 'shrewd-gerbil' - Using task runner 'ConcurrentTaskRunner'\n",
      "09:38:19.634 | WARNING | Flow run 'shrewd-gerbil' - No default storage is configured on the server. Results from this flow run will be stored in a temporary directory in its runtime environment.\n",
      "09:38:19.661 | INFO    | Flow run 'shrewd-gerbil' - Created task run 'apply_model-665e7535-0' for task 'apply_model'\n",
      "09:38:19.675 | INFO    | Task run 'apply_model-665e7535-0' - Reading the data from s3://nyc-tlc/trip data/green_tripdata_2021-02.parquet\n",
      "09:38:29.733 | INFO    | Task run 'apply_model-665e7535-0' - Loading model 1/f4e2242a53a3410d89c061d1958ae70a\n",
      "09:38:48.426 | INFO    | Task run 'apply_model-665e7535-0' - Applying the model\n",
      "09:38:48.816 | INFO    | Task run 'apply_model-665e7535-0' - Saving the result to batch_score.parquet\n",
      "09:38:49.135 | INFO    | Task run 'apply_model-665e7535-0' - Finished in state Completed()\n",
      "09:38:49.145 | INFO    | Flow run 'shrewd-gerbil' - Finished in state Completed('All states completed.')\n",
      "Completed(message='All states completed.', type=COMPLETED, result=[Completed(message=None, type=COMPLETED, result=None, task_run_id=27a60261-d23f-435f-b854-d7afacc58fc6)], flow_run_id=83e80f7c-1d8a-4bf3-9453-60dd40bd555b)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from prefect import flow\n",
    "from score import apply_model\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "@flow\n",
    "def test_apply_model():\n",
    "    apply_model(\n",
    "        input_file='s3://nyc-tlc/trip data/green_tripdata_2021-02.parquet', \n",
    "        output_file='batch_score.parquet', \n",
    "        experiment_id=os.getenv(\"EXPERIMENT_ID\"), \n",
    "        run_id=os.getenv(\"RUN_ID\")\n",
    "    )\n",
    "\n",
    "test_apply_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>actual_duration</th>\n",
       "      <th>predicted_duration</th>\n",
       "      <th>diff</th>\n",
       "      <th>model_version</th>\n",
       "      <th>ride_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-01 00:34:03</td>\n",
       "      <td>130</td>\n",
       "      <td>205</td>\n",
       "      <td>3.66</td>\n",
       "      <td>17.916667</td>\n",
       "      <td>18.210771</td>\n",
       "      <td>-0.294104</td>\n",
       "      <td>f4e2242a53a3410d89c061d1958ae70a</td>\n",
       "      <td>93bfe84a-800f-4ec7-b9e1-0966e1c57f21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-01 00:04:00</td>\n",
       "      <td>152</td>\n",
       "      <td>244</td>\n",
       "      <td>1.10</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>7.010569</td>\n",
       "      <td>-0.510569</td>\n",
       "      <td>f4e2242a53a3410d89c061d1958ae70a</td>\n",
       "      <td>b4cf270f-8324-4afe-98f8-69a3017d7c4b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-01 00:18:51</td>\n",
       "      <td>152</td>\n",
       "      <td>48</td>\n",
       "      <td>4.93</td>\n",
       "      <td>15.250000</td>\n",
       "      <td>21.430337</td>\n",
       "      <td>-6.180337</td>\n",
       "      <td>f4e2242a53a3410d89c061d1958ae70a</td>\n",
       "      <td>b93afee8-bac2-4c80-83c0-289bc4aa7fa4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-01 00:53:27</td>\n",
       "      <td>152</td>\n",
       "      <td>241</td>\n",
       "      <td>6.70</td>\n",
       "      <td>18.233333</td>\n",
       "      <td>24.683903</td>\n",
       "      <td>-6.450570</td>\n",
       "      <td>f4e2242a53a3410d89c061d1958ae70a</td>\n",
       "      <td>f7621d8f-394d-4ffd-923f-b30e6195f608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-01 00:57:46</td>\n",
       "      <td>75</td>\n",
       "      <td>42</td>\n",
       "      <td>1.89</td>\n",
       "      <td>8.966667</td>\n",
       "      <td>10.630539</td>\n",
       "      <td>-1.663872</td>\n",
       "      <td>f4e2242a53a3410d89c061d1958ae70a</td>\n",
       "      <td>44734c3d-9f44-46be-86bd-254afd4bdb07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lpep_pickup_datetime  PULocationID  DOLocationID  trip_distance  \\\n",
       "0  2021-02-01 00:34:03           130           205           3.66   \n",
       "1  2021-02-01 00:04:00           152           244           1.10   \n",
       "2  2021-02-01 00:18:51           152            48           4.93   \n",
       "3  2021-02-01 00:53:27           152           241           6.70   \n",
       "4  2021-02-01 00:57:46            75            42           1.89   \n",
       "\n",
       "   actual_duration  predicted_duration      diff  \\\n",
       "0        17.916667           18.210771 -0.294104   \n",
       "1         6.500000            7.010569 -0.510569   \n",
       "2        15.250000           21.430337 -6.180337   \n",
       "3        18.233333           24.683903 -6.450570   \n",
       "4         8.966667           10.630539 -1.663872   \n",
       "\n",
       "                      model_version                               ride_id  \n",
       "0  f4e2242a53a3410d89c061d1958ae70a  93bfe84a-800f-4ec7-b9e1-0966e1c57f21  \n",
       "1  f4e2242a53a3410d89c061d1958ae70a  b4cf270f-8324-4afe-98f8-69a3017d7c4b  \n",
       "2  f4e2242a53a3410d89c061d1958ae70a  b93afee8-bac2-4c80-83c0-289bc4aa7fa4  \n",
       "3  f4e2242a53a3410d89c061d1958ae70a  f7621d8f-394d-4ffd-923f-b30e6195f608  \n",
       "4  f4e2242a53a3410d89c061d1958ae70a  44734c3d-9f44-46be-86bd-254afd4bdb07  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('batch_score.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.713046656895089"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['diff'].values ** 2).mean() ** 0.5  # rmse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefect flow for batch scoring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we create the `flow` for batch scoring the data from the previous month relative to its scheduled run date. This will be deployed to run monthly for batch scoring rides that occured during the previous month. \n",
    "\n",
    "For example, if a `flow` is scheduled on `2021/06/02`, then batch scoring is performed on taxi data gathered on May 2021. The link to the correct file in S3 is returned by the `get_paths` function. Note that the files are already nicely aggregated by month and can be downloaded without worrying about credentials. For the output file, we simply separate outputs using different folders for each parameter value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`score.py`](https://github.com/particle1331/ok-transformer/blob/356f05bc25702efe41d80d4158f5b33864ae14a3/docs/nb/mlops/04-deployment/score.py#L41-L62)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_paths(run_date, taxi_type, run_id):\n",
    "    \"\"\"Get input and output file paths from scheduled date.\"\"\"\n",
    "\n",
    "    # Get previous month and year from run date\n",
    "    # e.g. run date=2021/06/02 -> month=5, year=2021 (input).\n",
    "    prev_month = run_date - relativedelta(months=1)\n",
    "    year = prev_month.year\n",
    "    month = prev_month.month \n",
    "\n",
    "    input_file = (\n",
    "        f's3://nyc-tlc/trip data/'\n",
    "        f'{taxi_type}_tripdata_'\n",
    "        f'{year:04d}-{month:02d}.parquet'\n",
    "    )\n",
    "    \n",
    "    output_file = (\n",
    "        f's3://nyc-duration-prediction-ron/' \n",
    "        f'taxi_type={taxi_type}/'\n",
    "        f'year={year:04d}/month={month:02d}/'{run_id}.parquet'\n",
    "    )\n",
    "\n",
    "    return input_file, output_file\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the `ride_duration_prediction` flow. From the `get_paths` function, we know that this flow applies the model on data collected over the previous month relative to the `run_date`. Below on our deployment specification, we leave out `run_date` so that it evaluates to `ctx.flow_run.expected_start_time` which fetches the scheduled run date of the flow. \n",
    "\n",
    "**Remark.** Getting scheduled run date makes sense because this delegates the responsibility of getting the correct date to the orchestrator instead of the executing machine, e.g. using `datetime` as the system clocks may be rogue or be in a different timezone, or these workflows can be delayed, resulting in output files containing predictions on wrong dates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`score.py`](https://github.com/particle1331/ok-transformer/blob/356f05bc25702efe41d80d4158f5b33864ae14a3/docs/nb/mlops/04-deployment/score.py#L85-L128)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@flow\n",
    "def ride_duration_prediction(\n",
    "        taxi_type: str,\n",
    "        run_id: str,\n",
    "        experiment_id: str,\n",
    "        run_date: datetime = None\n",
    "    ) -> None:\n",
    "\n",
    "    # Get scheduled data if no run_date\n",
    "    if run_date is None:\n",
    "        ctx = get_run_context()\n",
    "        run_date = ctx.flow_run.expected_start_time\n",
    "    \n",
    "    # Get input path and output path in S3\n",
    "    input_file, output_file = get_paths(run_date, taxi_type, run_id)\n",
    "\n",
    "    # Execute make predictions on input task\n",
    "    apply_model(\n",
    "        input_file=input_file,\n",
    "        output_file=output_file,\n",
    "        run_id=run_id,\n",
    "        experiment_id=experiment_id\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ...\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Run flow\n",
    "    ride_duration_prediction(\n",
    "        taxi_type=args.taxi_type,\n",
    "        run_id=args.run_id,\n",
    "        experiment_id=args.experiment_id,\n",
    "        run_date=datetime(year=args.year, month=args.month, day=2)\n",
    "    )\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running one flow on the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:07:39.115 | INFO    | prefect.engine - Created flow run 'piquant-jacamar' for flow 'ride-duration-prediction'\n",
      "12:07:39.115 | INFO    | Flow run 'piquant-jacamar' - Using task runner 'ConcurrentTaskRunner'\n",
      "12:07:39.120 | WARNING | Flow run 'piquant-jacamar' - No default storage is configured on the server. Results from this flow run will be stored in a temporary directory in its runtime environment.\n",
      "12:07:39.144 | INFO    | Flow run 'piquant-jacamar' - Created task run 'apply_model-b21fdc82-0' for task 'apply_model'\n",
      "12:07:39.157 | INFO    | Task run 'apply_model-b21fdc82-0' - Reading the data from s3://nyc-tlc/trip data/green_tripdata_2021-01.parquet\n",
      "12:07:48.615 | INFO    | Task run 'apply_model-b21fdc82-0' - Loading model 1/f4e2242a53a3410d89c061d1958ae70a\n",
      "12:08:08.153 | INFO    | Task run 'apply_model-b21fdc82-0' - Applying the model\n",
      "12:08:08.463 | INFO    | Task run 'apply_model-b21fdc82-0' - Saving the result to s3://nyc-duration-prediction-ron/taxi_type=green/year=2021/month=01/f4e2242a53a3410d89c061d1958ae70a.parquet\n",
      "12:08:19.979 | INFO    | Task run 'apply_model-b21fdc82-0' - Finished in state Completed()\n",
      "12:08:19.987 | INFO    | Flow run 'piquant-jacamar' - Finished in state Completed('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "!python score.py \\\n",
    "    --taxi-type green \\\n",
    "    --run-id f4e2242a53a3410d89c061d1958ae70a \\\n",
    "    --experiment-id 1 \\\n",
    "    --year 2021 \\\n",
    "    --month 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/prefect_scoring.png\n",
    "---\n",
    "---\n",
    "Prefect UI for a run of our batch scoring flow `ride_duration_prediction` with `run_date` set to Feb 2, 2021. This consists of a single task.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/output-s3.png\n",
    "---\n",
    "---\n",
    "The resulting output file in S3 for a flow with `run_date` set to Feb 2, 2021.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the batch scoring workflow in Prefect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create local storage in the terminal:\n",
    "\n",
    "```bash\n",
    "$ prefect storage create\n",
    "...\n",
    "Registered storage 'localstorage' with identifier '4c53e1c9-e8dc-4325-8832-2802e1778654'.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the following deployment specification:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`score_deploy.py`](https://github.com/particle1331/ok-transformer/blob/1b235cd046e6413be7a3b1701696e15399bd26bc/docs/nb/mlops/04-deployment/score_deploy.py)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from prefect.deployments import DeploymentSpec\n",
    "from prefect.orion.schemas.schedules import CronSchedule\n",
    "from prefect.flow_runners import SubprocessFlowRunner\n",
    "\n",
    "\n",
    "DeploymentSpec(\n",
    "    name=\"monthly\",\n",
    "    flow_location=\"./score.py\",\n",
    "    flow_name=\"ride-duration-prediction\",\n",
    "    parameters={\n",
    "        \"taxi_type\": \"green\",\n",
    "        \"run_id\": \"f4e2242a53a3410d89c061d1958ae70a\",\n",
    "        \"experiment_id\": \"1\",\n",
    "    },\n",
    "    flow_storage=\"4c53e1c9-e8dc-4325-8832-2802e1778654\",\n",
    "    schedule=CronSchedule(cron=\"0 3 2 * *\"), # https://crontab.guru/#0_3_2_*_*\n",
    "    flow_runner=SubprocessFlowRunner(),\n",
    "    tags=[\"ml\"]\n",
    ")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading deployment specifications from python script at \u001b[32m'score_deploy.py'\u001b[0m...\n",
      "/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/prefect/deployments.py:247: UserWarning: You have configured local storage, this deployment will only be usable from the current machine..\n",
      "  warnings.warn(\n",
      "Creating deployment \u001b[1;34m'monthly'\u001b[0m for flow \u001b[34m'ride-duration-prediction'\u001b[0m...\n",
      "Deploying flow script from \u001b[32m'/Users/particle1331/code/ok-transformer/docs/n\u001b[0m\n",
      "\u001b[32motebooks/mlops/04-deployment/score.py'\u001b[0m using Local Storage...\n",
      "Created deployment \u001b[34m'ride-duration-prediction/\u001b[0m\u001b[1;34mmonthly'\u001b[0m.\n",
      "View your new deployment with: \n",
      "\n",
      "    prefect deployment inspect \u001b[34m'ride-duration-prediction/\u001b[0m\u001b[1;34mmonthly'\u001b[0m\n",
      "\u001b[32mCreated 1 deployments!\u001b[0m\n",
      "\u001b[3m                                Deployments                                \u001b[0m\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
      "â”ƒ\u001b[1m \u001b[0m\u001b[1mName                            \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mID                                  \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
      "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
      "â”‚\u001b[34m \u001b[0m\u001b[34mride-duration-prediction/\u001b[0m\u001b[1;34mmonthly\u001b[0m\u001b[34m \u001b[0mâ”‚\u001b[36m \u001b[0m\u001b[36md20eb66c-6ddd-4045-b99f-a5f927f4f6ad\u001b[0m\u001b[36m \u001b[0mâ”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "!prefect deployment create score_deploy.py\n",
    "!prefect deployment ls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating work queue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mUUID\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'f60594a4-1879-47a5-8fdf-cc59f7a3f2f2'\u001b[0m\u001b[1m)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!prefect work-queue create \\\n",
    "    --deployment d20eb66c-6ddd-4045-b99f-a5f927f4f6ad \\\n",
    "    --flow-runner subprocess ride-duration-prediction-monthly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking runs for the next three months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
      "â”ƒ\u001b[1m \u001b[0m\u001b[1mScheduled Stâ€¦\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mRun ID                  \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mName \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mDeployment ID            \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
      "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
      "â”‚\u001b[33m \u001b[0m\u001b[33m2022-09-02 0â€¦\u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[36m \u001b[0m\u001b[36m5850a16a-a6d6-4beb-9b8bâ€¦\u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mdangâ€¦\u001b[0m\u001b[32m \u001b[0mâ”‚\u001b[34m \u001b[0m\u001b[34md20eb66c-6ddd-4045-b99f-â€¦\u001b[0m\u001b[34m \u001b[0mâ”‚\n",
      "â”‚\u001b[33m \u001b[0m\u001b[33m2022-08-02 0â€¦\u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[36m \u001b[0m\u001b[36m3dfd963d-dbff-46c4-bfccâ€¦\u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mnostâ€¦\u001b[0m\u001b[32m \u001b[0mâ”‚\u001b[34m \u001b[0m\u001b[34md20eb66c-6ddd-4045-b99f-â€¦\u001b[0m\u001b[34m \u001b[0mâ”‚\n",
      "â”‚\u001b[33m \u001b[0m\u001b[33m2022-07-02 0â€¦\u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[36m \u001b[0m\u001b[36m19c24867-851a-4adc-98d7â€¦\u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mgracâ€¦\u001b[0m\u001b[32m \u001b[0mâ”‚\u001b[34m \u001b[0m\u001b[34md20eb66c-6ddd-4045-b99f-â€¦\u001b[0m\u001b[34m \u001b[0mâ”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\u001b[31m                            (**) denotes a late run                             \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!prefect work-queue preview f60594a4-1879-47a5-8fdf-cc59f7a3f2f2 --hours $(( 24 * 90 ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the work-queue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting agent with ephemeral API...\n",
      "\n",
      "  ___ ___ ___ ___ ___ ___ _____     _   ___ ___ _  _ _____\n",
      " | _ \\ _ \\ __| __| __/ __|_   _|   /_\\ / __| __| \\| |_   _|\n",
      " |  _/   / _|| _|| _| (__  | |    / _ \\ (_ | _|| .` | | |\n",
      " |_| |_|_\\___|_| |___\\___| |_|   /_/ \\_\\___|___|_|\\_| |_|\n",
      "\n",
      "\n",
      "Agent started! Looking for work from queue \n",
      "'f60594a4-1879-47a5-8fdf-cc59f7a3f2f2'...\n",
      "^C\n",
      "\n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!prefect agent start f60594a4-1879-47a5-8fdf-cc59f7a3f2f2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/prefect-schedule.png\n",
    "---\n",
    "---\n",
    "Deployed workflows.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backfilling or predicting on previous dates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of the is to execute batch prediction subflows for months in the past using the current model. In the following flow, we simply execute the `ride_duration_prediction` flow with `run_date` for each month from `start_date` up to `end_date`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`score_backfill.py`](https://github.com/particle1331/ok-transformer/blob/caa2c81cd130ff8c027e31baeee8c4b56f8e88f2/docs/nb/mlops/04-deployment/score_backfill.py#L9-L56)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@flow\n",
    "def ride_duration_prediction_backfill(\n",
    "    run_id: str, \n",
    "    experiment_id: str,\n",
    "    taxi_type: str, \n",
    "    start_date: datetime, \n",
    "    end_date: datetime\n",
    "):\n",
    "    \"\"\"Run batch scoring flows for run dates \n",
    "    between start_date and end_date (inclusive).\"\"\"\n",
    "\n",
    "    run_date = start_date\n",
    "\n",
    "    while run_date <= end_date:\n",
    "\n",
    "        score.ride_duration_prediction(\n",
    "            taxi_type=taxi_type,\n",
    "            experiment_id=experiment_id,\n",
    "            run_id=run_id,\n",
    "            run_date=run_date\n",
    "        )\n",
    "        \n",
    "        run_date = run_date + relativedelta(months=1)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ...\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    start_date = datetime(year=args.start_year, month=args.start_month, day=1)\n",
    "    end_date = datetime(year=args.end_year, month=args.end_month, day=1)\n",
    "\n",
    "    ride_duration_prediction_backfill(\n",
    "        experiment_id=args.experiment_id,\n",
    "        run_id=args.run_id,\n",
    "        taxi_type=args.taxi_type,\n",
    "        start_date=start_date, \n",
    "        end_date=end_date\n",
    "    )\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running predictions on months December 2020 to March 2022:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:11:32.073 | INFO    | prefect.engine - Created flow run 'daffodil-bonobo' for flow 'ride-duration-prediction-backfill'\n",
      "13:11:32.073 | INFO    | Flow run 'daffodil-bonobo' - Using task runner 'ConcurrentTaskRunner'\n",
      "13:11:32.078 | WARNING | Flow run 'daffodil-bonobo' - No default storage is configured on the server. Results from this flow run will be stored in a temporary directory in its runtime environment.\n",
      "13:11:32.119 | INFO    | Flow run 'daffodil-bonobo' - Created subflow run 'chocolate-butterfly' for flow 'ride-duration-prediction'\n",
      "13:11:32.142 | INFO    | Flow run 'chocolate-butterfly' - Created task run 'apply_model-665e7535-0' for task 'apply_model'\n",
      "13:11:32.155 | INFO    | Task run 'apply_model-665e7535-0' - Reading the data from s3://nyc-tlc/trip data/green_tripdata_2020-12.parquet\n",
      "13:11:45.139 | INFO    | Task run 'apply_model-665e7535-0' - Loading model 1/f4e2242a53a3410d89c061d1958ae70a\n",
      "13:14:20.314 | INFO    | Task run 'apply_model-665e7535-0' - Applying the model\n",
      "13:14:20.642 | INFO    | Task run 'apply_model-665e7535-0' - Saving the result to s3://nyc-duration-prediction-ron/taxi_type=green/year=2020/month=12/f4e2242a53a3410d89c061d1958ae70a.parquet\n",
      "13:14:35.829 | INFO    | Task run 'apply_model-665e7535-0' - Finished in state Completed()\n",
      "13:14:35.843 | INFO    | Flow run 'chocolate-butterfly' - Finished in state Completed('All states completed.')\n",
      "13:14:35.872 | INFO    | Flow run 'daffodil-bonobo' - Created subflow run 'esoteric-caterpillar' for flow 'ride-duration-prediction'\n",
      "13:14:35.895 | INFO    | Flow run 'esoteric-caterpillar' - Created task run 'apply_model-665e7535-1' for task 'apply_model'\n",
      "13:14:35.905 | INFO    | Task run 'apply_model-665e7535-1' - Reading the data from s3://nyc-tlc/trip data/green_tripdata_2021-01.parquet\n",
      "13:14:44.977 | INFO    | Task run 'apply_model-665e7535-1' - Loading model 1/f4e2242a53a3410d89c061d1958ae70a\n",
      "13:16:15.032 | INFO    | Task run 'apply_model-665e7535-1' - Applying the model\n",
      "13:16:15.383 | INFO    | Task run 'apply_model-665e7535-1' - Saving the result to s3://nyc-duration-prediction-ron/taxi_type=green/year=2021/month=01/f4e2242a53a3410d89c061d1958ae70a.parquet\n",
      "13:16:27.319 | INFO    | Task run 'apply_model-665e7535-1' - Finished in state Completed()\n",
      "13:16:27.331 | INFO    | Flow run 'esoteric-caterpillar' - Finished in state Completed('All states completed.')\n",
      "..."
     ]
    }
   ],
   "source": [
    "!python score_backfill.py \\\n",
    "    --taxi-type green \\\n",
    "    --experiment-id 1 \\\n",
    "    --run-id f4e2242a53a3410d89c061d1958ae70a \\\n",
    "    --start-month 1 \\\n",
    "    --start-year 2021 \\\n",
    "    --end-month 4 \\\n",
    "    --end-year 2022 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/prefect-backfill.png\n",
    "---\n",
    "---\n",
    "Running batch prediction tasks for 16 months.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/batch-s3.png\n",
    "---\n",
    "---\n",
    "Output files for 16 months of batch predictions.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Model train script"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training models that we use to serve predictions in our API, we use the following script. This trains a model using the `ride_duration` package (which ensures smooth integration with the Flask API) and logs the trained model to a remote MLflow tracking server. The tracking server host is provided as a command line argument."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`train.py`](https://github.com/particle1331/ok-transformer/blob/217134c84bb323452bf0dc3e8b6a6a04fea8f06b/docs/nb/mlops/04-deployment/train.py)\n",
    "```\n",
    "```python\n",
    "import mlflow \n",
    "import joblib\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from ride_duration.utils import load_training_dataframe, prepare_features\n",
    "\n",
    "\n",
    "def setup(tracking_server_host):\n",
    "    TRACKING_URI = f\"http://{tracking_server_host}:5000\"\n",
    "    mlflow.set_tracking_uri(TRACKING_URI)\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "\n",
    "\n",
    "def run_training(X_train, y_train, X_valid, y_valid):\n",
    "    with mlflow.start_run():\n",
    "        params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 20\n",
    "        }\n",
    "        \n",
    "        pipeline = make_pipeline(\n",
    "            DictVectorizer(), \n",
    "            RandomForestRegressor(**params, n_jobs=-1)\n",
    "        )\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_valid)\n",
    "        rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
    "        \n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"rmse_valid\", rmse)\n",
    "        mlflow.sklearn.log_model(pipeline, artifact_path='model')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--tracking-server-host\", type=str)\n",
    "    parser.add_argument(\"--train-path\", type=str)\n",
    "    parser.add_argument(\"--valid-path\", type=str)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Getting data from disk\n",
    "    train_data = load_training_dataframe(args.train_path)\n",
    "    valid_data = load_training_dataframe(args.valid_path)\n",
    "\n",
    "    # Preprocessing dataset\n",
    "    X_train = prepare_features(train_data.drop(['duration'], axis=1))\n",
    "    X_valid = prepare_features(valid_data.drop(['duration'], axis=1))\n",
    "    y_train = train_data.duration.values\n",
    "    y_valid = valid_data.duration.values\n",
    "\n",
    "    # Push training to server\n",
    "    setup(args.tracking_server_host)\n",
    "    run_training(X_train, y_train, X_valid, y_valid)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a55a0d1272a360f93e747858d443ec26da69f69eac36db3e567a961ca624a861"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
